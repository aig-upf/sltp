\documentclass[12pt]{article}

\topmargin 0in
\headheight 0in
\headsep 0in
\oddsidemargin 0in
\evensidemargin 0in
\textheight 9in
\textwidth 6.5in
\parskip 2ex

\usepackage{color,soul}
\newcommand\hg[1]{\sethlcolor{green}\hl{HG: #1}\sethlcolor{cyan}}
\newcommand\bb[1]{\sethlcolor{cyan}\hl{BB: #1}}
\newcommand\gf[1]{\sethlcolor{yellow}\hl{GF: #1}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{upgreek}
\usepackage[normalem]{ulem}
\usepackage{natbib}


\newcommand{\Omit}[1]{}
\newcommand{\denselist}{\itemsep -1pt\partopsep 0pt}
\newcommand{\tup}[1]{\langle #1 \rangle}
\newcommand{\pair}[1]{\langle #1 \rangle}
\newcommand{\tuple}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\set}[1]{\ensuremath{\left\{#1 \right\}}}
\newcommand{\setst}[2]{\ensuremath{\left\{#1 \mid #2 \right\}}}
\newcommand{\abs}[1]{\ensuremath{\left\vert{#1}\right\vert}}
\newcommand{\citeay}[1]{\citeauthor{#1} [\citeyear{#1}]}
\newcommand{\citeaywithnote}[2]{\citeauthor{#1} (\citeyear[#2]{#1})}
\newcommand{\mli}[1]{\mathit{#1}} % Multi-letter identifier, see https://tex.stackexchange.com/a/129434

\newtheorem{definition}{Definition}
\newtheorem{definitionandtheorem}[definition]{Definition and Theorem}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{example}[definition]{Example}

\def\withproofs{0} % to include proofs in the draft: 0=don't include, 1=include
\newcommand{\nosuchproof}{\textcolor{red}{\bf No such proof yet...}}
\newcommand{\problem}{\textcolor{red}{\bf **** PROBLEM ****}}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\CHECK}[1]{\textcolor{red}{\bf Note: #1}}

\newcommand{\C}{\mathcal{C}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}


\newcommand{\true}{\mathsf{true}}
\newcommand{\false}{\mathsf{false}}
\newcommand{\Eff}{{\mathit{Eff}}}

\newcommand{\abst}[2]{\tup{#1;#2}}
\newcommand{\Rule}[2]{\ensuremath{#1 \Rightarrow #2}}

% decrements and increments
\newcommand{\pplus}{\hspace{-.05em}\raisebox{.15ex}{\footnotesize$\uparrow$}}
\newcommand{\mminus}{\hspace{-.05em}\raisebox{.15ex}{\footnotesize$\downarrow$}}

% numerical effects
\newcommand{\dec}[1]{#1:=#1-1}
\newcommand{\inc}[1]{#1:=#1+1}

% qualitative effects
\newcommand{\Dec}{Dec}
\newcommand{\Inc}{Inc}
\newcommand{\RInc}{\ensuremath{\text{Inc}^*}}  % relaxed increase

% STRIPS actions
\newcommand{\Stack}{\text{Stack}}
\newcommand{\Unstack}{\text{Unstack}}
\newcommand{\Pickup}{\text{Pickup}}
\newcommand{\Putdown}{\text{Putdown}}
\newcommand{\Move}{\text{Move}}

% Examples
\newcommand{\Example}{\medskip\noindent\textbf{Example.}\xspace}


\begin{document}

\begin{center}
  \textbf{\large Learning Relational Features for Generalized Planning} \\
  BB, GF, HG; Revised \today
\end{center}


\section{Examples}


\subsection{Clearing a Block in Blocksworld}
We want to achieve a goal $clear(x)$ for a certain block $x$, starting from
any block configuration, where $x$ is a domain parameter.
When in validation mode, our approach validates
the feature set described in \citep{bonet2018features} as being sound and complete.
That feature set has a total aggregated complexity of $33$, and its most complex feature,
labelled as $m(x)$ (which denotes the set of blocks in any state which
is not in the same tower as block $x$ and is not being held either), has complexity $18$.


\paragraph{Learning on $3$ blocks.}
If we feed our approach with the full state space of a 3-block blocksworld, we learn an abstract space based on 5 features (total complexity: 12):

\begin{itemize}
 \item $F_1 \equiv holding$: whether some block is being held.
 \item $F_2 \equiv ontable \sqcap \set{x}$: whether block $x$ is on the table.
 \item $F_3 \equiv clear \sqcap \set{x}$: whether block $x$ is clear.
 
 \item $F_4 \equiv \abs{\neg ontable}$: Number of blocks not in the table.
 \item $F_5 \equiv \abs{ontable \sqcap clear}$: Number of blocks that are clear and on the table.
 
\end{itemize}


\noindent
The abstract action model that derives from these features is overfit to the fact that there are only three blocks.
% An example is the use of the condition $\neg F_1 \land (F_5 = 0)$ as a simple surrogate for the more complex concept
% that would describe that ``all blocks are on a single tower'' --- only if we have a number of blocks $n \leq 3$ both 
% concepts describe the same thing.
An example is the use of the condition $\neg F_1 \land \neg F_2 \land \neg F_3$ as a surrogate for the 
condition ``there is one single block on top of block $x$''. The latter can also be expressed with concept-based
features, but they have higher complexity, hence our approach prefers the former (this is because 
with only 3 blocks, if $x$ is not being held, not on the table and not clear, then it must be in the middle
of a 3-block tower). We hence learn an action model that is sound with respect to the samples, but not sound \emph{in general}.


\paragraph{Learning on $4$ blocks.}
If we expand the full 4-block state space, 
 we learn an abstract space based on 7 features (total complexity: 20):

 
 
 
\begin{itemize}
 \item $F_1 \equiv holding$: whether some block is being held.
 \item $F_2 \equiv ontable \sqcap \set{x}$: whether block $x$ is on the table.
 \item $F_3 \equiv holding \sqcap \set{x}$: whether block $x$ is being held.
 \item $F_4 \equiv \abs{\exists on . \top}$: Number of blocks that are on top of some other block.
 \item $F_5 \equiv \abs{\exists on^* . \set{x}}$: Number of blocks on top of block $x$.
 \item $F_6 \equiv \abs{\exists on . \neg ontable}$: Number of blocks that have at least two blocks below.
 \item $F_7 \equiv \abs{ontable \sqcap clear}$: Number of blocks that are clear and on the table.


\end{itemize}
  
\noindent
$F_5$ is the concept $n(x)$ from \citep{bonet2018features}, but the set of features does not generalize well.
\gf{(Haven't figured out the details, but checked it with our approach on larger BW instances and doesn't work)}

\paragraph{Learning on $5$ blocks with random sampling.}

Only with $5$ blocks, a similar set of features that that of \citep{bonet2018features}, with 5 features, turns out to be the simplest
which is sound and complete. The total complexity of the set is 26:


\begin{itemize}
 \item $F_1 \equiv handempty$: whether the hand is empty.
 \item $F_2 \equiv ontable \sqcap \set{x}$: whether block $x$ is on the table.
 \item $F_3 \equiv holding \sqcap \set{x}$: whether block $x$ is being held.
 \item $F_4 \equiv \abs{\exists on^* . \set{x}}$: Number of blocks on top of block $x$.
 \item $F_5 \equiv \abs{C_1 \sqcap C_2 \sqcap \neg \set{x}}$,
\end{itemize}

\noindent where 
$C_1 \equiv \forall on^* . \neg \set{x}$ denotes the set of blocks which are not above $x$,
$C_2 \equiv \forall (on^{-1})^* . \neg \set{x}$ the set of blocks which are not below $x$,
and the whole conjunction $F_5$ thus denotes the set of blocks not in the same tower as $x$.


This abstraction we computed by using a slightly different approach, in which the whole state space is expanded, but then
a certain number of states are \textbf{randomly sampled}, with the idea of providing a more diverse set of situations, for a fixed number
of states and transitions. In this case, with as few as $40$ state expansions, we obtain the reported abstraction.
The space of all concept-based features of size $k \leq 15$, which is the size of the feature $F_5$ above,
contains a total of $6858$ non-redundant features.
The resulting max-sat problem features $20006$ variables and $25.99$ million clauses.

\paragraph{Learning on $5$ blocks with optimal-state marking.}
An alternative to scale up is to pursue completeness only with respect to states lying on some optimal path.



\subsection{Blocksworld: $on(x, y)$ from 2 Towers}

Let's assume we want to achieve a goal $on(x, y)$, where $x$ and $y$ are domain parameters which denote
blocks that start in two different towers.

We here use directly the optimal-state marking strategy.

We train the approach with two small instances:
\begin{enumerate}
 \item One where we are holding block block $x$, and we have a single tower $D C B$ (i.e. D is on C, which is on B, and B is on the table).
 \item Another instance where we have two towers. Tower \#1 is B D C, tower \#2 is F E A.
\end{enumerate}

The abstract space learnt has 6 features (total aggregated complexity: 20)

\begin{itemize}
 \item $handempty$.
 \item $n(x)$: Number of blocks above $x$.
 \item $n(y)$: Number of blocks above $x$.
 \item $holding(x)$.
 \item $ontable(x)$.
 \item $F_6 \equiv \set{x} \sqcap (\forall on . \set{y})$: Whether block $x$ is either on $y$, on the table, or held.
\end{itemize}

The feature $ontable(x)$ is necessary to properly identify the goal, which is $F_6 \land handempty \land \neg ontable(x)$.
\gf{Not totally what we wanted, but looks good}

(\gf{Experiment ID: bw\_on\_x\_y\_completeness\_opt})


\Omit{
\subsection{Creating a Tower in Blocksworld}

With a 5-block state space, and using concepts up to size 15, the abstraction we find overfits
to the fact that there are only that many blocks. To illustrate, the existence of a tower
of size $3$ can be expressed with the feature
$\abs{\exists on . (\exists on . ontable)} > 0$,
that of a tower of size $4$, with the concept
$\abs{\exists on .\left[ \exists on . (\exists on . ontable)\right]} > 0$, and so on.

Interestingly, expressing the notion of ``a single tower'' using our concept-based language turns out to be harder
that expression the notion of ``a single tower with block $x$ at the bottom''. Whereas no truly generalized expression
for the first is found through our approach with a maximum feature size of 15, the feature 
$$\abs{\neg \set{x} \sqcap (\forall on^* . \neg \set{x})},$$
%     not_x_and_all_below_not_x = AndConcept(not_table, AndConcept(all_below_not_x, not_x, "object"), "object")
\noindent denotes how many blocks $y \neq x$ are not above $x$ (when this is 0, we must necessarily
have a tower based on $x$), and has only size $8$.
}

\subsection{Gripper}

In Gripper a robot with two grippers must move a number of balls which are in a certain room $A$ to a certain target room $B$,
every gripper being only able to hold one single ball. We use the standard encoding of Gripper with 
\emph{move}, \emph{pick} and \emph{drop} actions, and predicates that denote the location of balls and robot,
and whether each gripper is holding some ball or is empty. We consider the target room $x$ as the only domain parameter.
Distance features play no role in this problem, as all rooms are connected with the target room.

We learn on a small instance with only 3 balls, and expanding the full state space with 88 states and 280 transitions.
With a maximum feature size of 10 and a max. of
3 iterations of the concept-generating grammar, our approach learns an abstract space based on the following features:\footnote{
Interestingly, the max-sat compilation is quite large, in the order of roughly 400K variables and 40 million clauses, but the characteristics
of the state space allow the max-sat solver to solve it in around 20 seconds on a standard laptop.} 

\begin{itemize}
 \item $\abs{free}$: Number of empty grippers.
 \item $at\_robby \sqcap \set{x}$: Whether the robot is in the target room.
 \item $\exists at . \set{x}$: Number of balls in the target room.
 \item $\exists at . \neg \set{x}$: Number of balls not in the target room.
 \item $\exists carry . \top$: Number of balls being held.
\end{itemize}

The abstract state space results in an abstract action model which is sound and complete in general.
\gf{double-check}

\gf{I also have a generalization of gripper with $n$ rooms and $m$ robots, but haven't tested it thoroughly. Might make for a more interesting discussion.}


        
\subsection{Moving in a Grid}

An agent must move between two cells in a given bidimensional grid, with
the position of the represented as a pair $(x, y)$ of Cartesian coordinates in the range $[1, M]$,
and we use the standard natural successor relation $succ(a, b)$ to denote that $b = a + 1$.
From a simple $5 \times 5$ grid where the agent needs to go from one corner to the opposite one, 
we learn an abstract state space 
with two features
$\abs{\exists succ^*. x}$ and
$\abs{\exists (succ^{-1})^*. x}$
(and likewise for coordinate $y$, for a total of four features),
which denote, respectively, the distance between the current $x$ coordinate and the left and right limits of the grid.

 


\subsection{Graph Traversal}

An agent must traverse an arbirtary graph from a source to a target node. The problem is encoded 
using a set of nodes plus an adjacency relation between nodes. When allowing distance features, our approach learns an abstract
state space made up of a single feature 
\[D = dist(at, adjacent, at_g)\]
that represents the minimum distance between the current and the target node. The abstract action model is:
\begin{enumerate}
 \item $\tuple{D>0; D\downarrow}$
 \item $\tuple{D>0; \RInc(D)}$
 \item $\tuple{D=0; \RInc(D)}$
\end{enumerate}


Using \emph{goal concepts} is of course required to generate this feature; without them, no abstract model is found.
Using a the relaxed \emph{increase} semantics is also required.
Depending on the topology of the graph, a \emph{no-op} concrete action might be necessary for the above abstract model to be sound.\footnote{
This happens when there is some node $v$ (e.g. in a line graph) from which the distance $D>0$ to the target node can not be increased, but this
node $v$ is abstracted into the same abstract state as other nodes $D>0$ for which it can be increased. In this case, the action above with effect
$\RInc(D)$ requires a concrete no-op in order to be sound.
}

\subsection{Collecting Rewards in a Grid}

An agent moves in a bidimensional grid collecting rewards that are on random cells of the grid. The goal is to collect all available reward.
The concrete actions allow the agent to move between any two adjacent cells, and to pick the reward on the current cell, if any (only one reward item per cell is allowed).
The encoding uses an adjacency relation, not a Cartesian coordinate system (the distinction is relevant).

The approach learns a simple abstraction with two features $R = \abs{reward}$ and $D = dist(at, adjacent, reward)$, the first denoting the number of uncollected rewards,
and the second the minimum distance between the current cell and any cell with reward. The resulting abstract action model has the following 5 actions (names are only a posteriori manual labels):

\begin{enumerate}
 \item $\text{pick-reward}: \tuple{R>0, D=0; \Dec(R), \RInc(D)}$
 
 \item $\text{move-to-reward}: \tuple{D>0; \Dec(D)}$
 \item $\text{move-away-from-reward}: \tuple{D=0, R>0; \RInc(D)}$
 \item $\text{move-otherwise}: \tuple{D>0; \RInc(D)}$
\end{enumerate}



This example however illustrates the shortcomings of the relaxed increase semantics. 
If we discard the previous solution, another more complex state space is learnt, based only on the feature
\[D = dist(at, adjacent, T), \]

\noindent where the concept $T \equiv \exists adjacent^*.reward$ denotes the set of all cells from which the reward is reachable.
The distance $D$ is thus always $0$, except when no reward is left on the grid, in which case it is $\infty$. This allows to distinguish between goal
and non-goal states, but the resulting abstract action model is:
\begin{enumerate}
 \item $\tuple{D>0; \RInc(D)}$
 \item $\tuple{D=0; \RInc(D)}$
\end{enumerate}

which is not too useful. The problem here is that $D$ can only increase from $0$ to $\infty$, and the relaxed INC semantics
allow this set of abstract actions to be sound.



{
\small
\bibliographystyle{named}
\bibliography{control}
}

\end{document}

