\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{booktabs}  %Required

\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required

\setcounter{secnumdepth}{0}  

% \usepackage{soul}
% \usepackage[hidelinks]{hyperref}
% \usepackage[utf8]{inputenc}
% \usepackage[small]{caption}
%\usepackage{helvet}
%\usepackage{graphicx}
%\usepackage{url}

%\usepackage{color,soul}
%\newcommand\sr[1]{\sethlcolor{yellow}\hl{SR: #1}\sethlcolor{cyan}}
%\newcommand\hg[1]{\sethlcolor{green}\hl{HG: #1}\sethlcolor{cyan}}
%\newcommand\gdg[1]{\sethlcolor{yellow}\hl{GDG: #1}\sethlcolor{cyan}}
%\newcommand\bb[1]{\sethlcolor{cyan}\hl{BB: #1}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
%\usepackage{upgreek}
%\usepackage[normalem]{ulem}
\usepackage{multirow}
% \usepackage[small]{caption}


\usepackage{xspace}
% \usepackage{ifthen}
\usepackage{tikz}
% \usetikzlibrary{shapes.geometric}
% \pgfdeclareimage[width=1cm]{dirt}{dirt2}

% \usetikzlibrary{shapes.geometric}
% \pgfdeclareimage[width=1cm]{dirt}{dirt2}

\newcommand{\Omit}[1]{}
\newcommand{\denselist}{\itemsep -1pt\partopsep 0pt}
\newcommand{\tup}[1]{\langle #1 \rangle}
\newcommand{\pair}[1]{\langle #1 \rangle}
\newcommand{\tuple}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\set}[1]{\ensuremath{\left\{#1 \right\}}}
\newcommand{\setst}[2]{\ensuremath{\left\{#1 \mid #2 \right\}}}
\newcommand{\abs}[1]{\ensuremath{\left\vert{#1}\right\vert}}
% \newcommand{\citeay}[1]{\citeauthor{#1} [\citeyear{#1}]}
\newcommand{\citeay}[1]{\citeauthor{#1} (\citeyear{#1})}
\newcommand{\citeaywithnote}[2]{\citeauthor{#1} (\citeyear[#2]{#1})}


\newtheorem{definition}{Definition}
\newtheorem{definitionandtheorem}[definition]{Definition and Theorem}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{example}[definition]{Example}

\def\withproofs{0} % to include proofs in the draft: 0=don't include, 1=include
\newcommand{\nosuchproof}{\textcolor{red}{\bf No such proof yet...}}
\newcommand{\problem}{\textcolor{red}{\bf **** PROBLEM ****}}
\newcommand{\alert}[1]{\textcolor{red}{\bf #1}}
\newcommand{\CHECK}[1]{\textcolor{red}{\bf Note: #1}}

\newcommand{\hobs}{\ensuremath{h_{obs}}\xspace}
\newcommand{\indicator}[1]{[\![#1]\!]}
\newcommand{\C}{\mathcal{C}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\dexp}{\text{2-}\!\exp}

\newcommand{\Pn}{\ensuremath{P_n}\xspace}
\newcommand{\Pnm}{\ensuremath{P_{n,m}}\xspace}
\newcommand{\muwf}{\ensuremath{\mu_{w\!f}}\xspace}

\newcommand{\true}{\mathsf{true}}
\newcommand{\false}{\mathsf{false}}
\newcommand{\Eff}{{\mathit{Eff}}}

%% C++
\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
%%% \def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}
%%% \newcommand{\pplus}{\!\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
%%% \newcommand{\mminus}{\!\hspace{-.05em}\raisebox{.4ex}{\tiny\bf -}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf - }}
%%%\newcommand{\pplus}{\hspace{-.05em}\raisebox{.2ex}{\footnotesize\mit +}\nolinebreak\hspace{-.10em}\raisebox{.2ex}{\footnotesize\mit +}}
%%%\newcommand{\mminus}{\hspace{-.05em}\raisebox{.2ex}{\footnotesize\mit -}\nolinebreak\hspace{-.07em}\raisebox{.2ex}{\footnotesize\mit -}}

% abstract actions and plans
\newcommand{\abst}[2]{\tup{#1;#2}}
\newcommand{\Rule}[2]{\ensuremath{#1 \Rightarrow #2}}

% decrements and increments
\newcommand{\pplus}{\hspace{-.05em}\raisebox{.15ex}{\footnotesize$\uparrow$}}
\newcommand{\mminus}{\hspace{-.05em}\raisebox{.15ex}{\footnotesize$\downarrow$}}

% numerical effects
\newcommand{\dec}[1]{#1:=#1-1}
\newcommand{\inc}[1]{#1:=#1+1}

% qualitative effects
\newcommand{\Dec}{Dec}
\newcommand{\Inc}{Inc}

% STRIPS actions
\newcommand{\Stack}{\text{Stack}}
\newcommand{\Unstack}{\text{Unstack}}
\newcommand{\Pickup}{\text{Pickup}}
\newcommand{\Putdown}{\text{Putdown}}
\newcommand{\Move}{\text{Move}}

% Examples
\newcommand{\Example}{\medskip\noindent\textbf{Example.}\xspace}


%PDF Info Is Required (no accents, etc.!):
\pdfinfo{
/Title ()
/Author ()}


\title{Learning Features and Abstract Actions for Computing Generalized Plans} 

% \author{B. Bonet, G. Frances, H. Geffner} % Alphabetical
% 
\author{\textbf{Submission \#5695}}
\nocopyright


\begin{document}


\maketitle

\begin{abstract}
  Generalized planning is concerned with the computation of plans that solve not one but multiple instances
  of a class of planning problems.
  Recently, it has been shown that generalized plans can be expressed as mappings of feature
  values into actions, and that they can often be computed with fully observable non-deterministic
  (FOND) planners. %  on suitable FOND problems. %after a sequence of transformations.
  The actions in such plans, however, are not the actions in the instances themselves, which are
  not necessarily common to  other instances, but \emph{abstract} actions that are defined on a set 
  of \emph{common} features. The formulation assumes that both the features and the abstract
  actions are given.
  In this work, we address this limitation by showing how to learn them automatically. 
  %  the features and  abstract actions.
  The resulting account of generalized planning combines learning and planning
  in a novel way: a \emph{learner}, based on a min SAT formulation, yields the features and abstract
  actions from state transitions sampled from problem instances, and a FOND \emph{planner}
  uses this information, suitably transformed, to produce the general plans.
  Correctness guarantees are given and experimental results on several domains are reported.
\end{abstract}


\section{Introduction}

Generalized planning studies the computation of plans  that   solve   multiple  instances
\cite{srivastava08learning,bonet09automatic,srivastava:generalized,hu:generalized,BelleL16,anders:generalized}.
For example, the plan  that iteratively  picks  a  clear block above $x$ 
and places it  on the table, achieves the goal $clear(x)$
in \emph{any} instance of the Blocksworld where the gripper is initially empty.
Once this  general plan or  policy is derived it can be  applied to solve an infinite collection
of instances that involve different initial states, different objects, and hence  different (ground)  actions. 


In the basic formulation due  to \citeay{hu:generalized}, a generalized plan
is a mapping of observations into actions that are assumed to be common
among  all the instances. More recently, this formulation has been extended by
\citeay{bonet:ijcai2018} to account for relational domains like Blocksworld  where the set of
objects and  actions change from instance to instance.
In the new formulation, the observations  are replaced by  a set of boolean
and numerical features $F$ and a set of \emph{abstract actions} $A_F$.
The  abstract actions are  \emph{sound} and \emph{complete} if they track the
effects  of the actions on  the features in a suitable way.
The resulting generalized plans  map feature values into abstract actions, and soundness
ensures that the application of an abstract action can be mapped back into the application of
a concrete action with analog effect  on  the features. Moreover, the form of the abstract actions
ensures that generalized plans can be computed using fully observable non-deterministic (FOND) planners
once the generalized planning problem is transformed into a FOND problem.
%after some transformations.

\Omit{
Abstract actions can increase or decrease numerical variables  $n$
associated with the numerical features (e.g., number of blocks above $x$), and while
increments  are transformed into deterministic propositional  effects $n > 0$ where $n > 0$ is
the negation of the proposition $n=0$, decrements  are  transformed  into
non-deterministic (disjunctive)  propositional effects $n > 0 \, | \, n=0$. 
}

Bonet and Geffner's generalized planning formulation assumes that the
features and abstract actions are given. In this work we address this limitation
by  \emph{showing  how the features and abstract actions  can be learned from
the primitive predicates used to define the instances and a given sample of state transitions.}
For example,  the general policy for achieving $clear(x)$ % of \citeay{bonet:ijcai2018}
is obtained using a FOND planner on an abstraction that consists of  a  boolean
feature $H$  that tracks whether the gripper holds a block,  a numerical feature
$n(x)$ that counts the number of blocks above $x$, and two abstract actions:
one with preconditions $\neg H$ and $n(x) > 0$, and effects $H$ and $n(x)\mminus$
(decrement $n(x)$), and the other with precondition $H$ and effect $\neg H$.
In this work, we show how to obtain such policies from STRIPS instances alone,
without having to provide the features and abstract actions by hand.

The work has relation to a number of research threads in planning, knowledge
representation, and machine learning. We make use of SAT solvers and description
logics for learning features and abstract actions \cite{sat-handbook,dl-handbook}.
The abstract actions provide a model from which the plans are obtained via
transformations and FOND planners \cite{geffner:book,ghallab:book}.
The model is not the model of an instance but a generalized model for obtaining
plans that work for multiple instances. In this sense, the work is different than
action model learning \cite{yang:action-learning} and model-based reinforcement
learning and closer in aims to work on learning general policies from examples or
experience \cite{martin-geffner:generalized,fern:bias,mazebase,general-drl}.
Generalized planning has also been formulated as a problem in first-order logic
\cite{srivastava:generalized}, and general plans over finite horizons have been
derived using first-order regression \cite{boutilier2001symbolic,wang2008first,van2012solving}.
Our approach differs from first-order approaches in the use of propositional
planners, and from purely learning approaches, in the \emph{formal guarantees} that
are characteristic of planning: if the learned abstract actions are actually sound,
the resulting general plans are correct. 

The paper is organized as follows. We provide the background  first and show then
how features and abstract actions can be learned by enforcing soundness and
completeness over a set of samples  using features derived from the domain predicates.
The full computational model is then summarized, followed by experimental results
and a discussion.


\section{Background}

We review generalized planning, abstract actions, and computation of solutions 
following  \citeay{bonet:ijcai2018}.


\subsection{Generalized Planning}

A \emph{generalized planning problem} $\Q$ is a collection of planning instances $P$.
An instance $P$ is a  classical planning problem expressed in some compact language
as a tuple $P=\tup{V,I,G,A}$ where $V$ is a set of state variables that can take a
finite set of values (boolean or not), $I$ is a set of atoms over $V$ defining an
initial state $s_0$, $G$ is a set of atoms or literals over $V$ describing the goal
states, and $A$ is a set of actions $a$ with their preconditions and effects that
define the set $A(s)$ of actions applicable in any state $s$, and the successor
state function $f(a,s)$, for any state $s$ and $a \in A(s)$.
A state is a valuation over $V$, and a solution to $P$ is an applicable action
sequence $\pi=a_0,\ldots,a_n$ that generates  a state sequence $s_0,s_1,\ldots,s_{n}$
where $s_n$ is a goal state (makes $G$ true). In this sequence, $a_i \in A(s_i)$
and $s_{i+1}=f(a_i,s_i)$ for $i=0, \ldots, n-1$. A state $s$ is reachable in $P$
if $s=s_n$ for one such sequence. A solution to the \emph{generalized problem}
$\Q$ is a solution to all instances $P \in Q$. The form of such solutions is
described below.

A \emph{feature} $f$ for a class $\Q$ of problems represents a function $\phi_f$
that takes an instance $P$ from $\Q$ and a state $s$ reachable in $P$, and results
in a value denoted as $\phi_f(s)$. 
A feature is \emph{boolean} if it results in a boolean value, and \emph{numeric}
if it results in a numerical value, that we assume to be a non-negative integer.
$H$ and $n(x)$ represent two features in the Blocksworld:
the first is a boolean feature that tracks whether the gripper is empty, while
the second is numerical and tracks the number of blocks above $x$. 

Symbols like $x$ denote \emph{parameters} whose value depends on the
instance $P$ in $\Q$. For example, if $\Q_{clear}$ denotes the Blocksworld
instances with goals of the form $clear(x)$, the value of $x$ in an instance
$P$ with goal $clear(a)$ is $a$.


\subsection{Abstract Actions}

An \emph{abstract action}  for a generalized problem $\Q$ and a set of features $F$
is a pair $\bar{a}=\abst{Pre}{\Eff}$ where $Pre$ and $\Eff$ are the  preconditions
and effects expressed in terms of a  set $V_F$ of boolean and numerical \emph{state variables}
associated with the features in $F$. Preconditions and effects over boolean state variables $p$ are literals of the form $p$
and $\neg p$, that abbreviate the  atoms $p=true$ and $p=false$, while  over  numerical state variables $n$,
preconditions are of  the form $n=0$ and $n > 0$, and effects  of the form $n\mminus$ (decrements) and
$n\pplus$ (increments).

Features $f$ refer to \emph{state functions} $\phi_f(s)$ in the instances
of the generalized problem $\Q$, but to \emph{state variables} in the
abstraction. Likewise, an \emph{abstract state} is  a truth valuation over
the state variables $p$ associated with the boolean features and the atoms
$n=0$ over the state variables $n$ associated with the numerical features.
The abstract state $\bar{s}$ that \emph{corresponds} to a concrete state $s$
is the truth valuation that makes $p$ true iff $\phi_p(s)=true$, and $n=0$
true iff $\phi_n(s) = 0$. An abstract action $\bar{a}$ is applicable in
a state $s$ if its preconditions are satisfied in $\bar{s}$.

A set of abstract actions $A_F$ is \emph{sound} relative to a generalized
problem $\Q$ iff for any reachable state $s$ over an instance $P$ in $\Q$
and any abstract action $\bar{a}$ in $A_F$ that is applicable in $\bar{s}$,
there is a concrete action $a$ in $P$ that is applicable in $s$ and that
has the \emph{same qualitative effects as} $\bar{a}$ over the features.
The action $a$ \emph{represents} or \emph{instantiates} the abstract action
$\bar{a}$ on $s$. 
%
Similarly, a set of abstract actions $A_F$ is \emph{complete} iff for any
reachable state $s$ over an instance $P$ in $\Q$ and any concrete action $a$
in $P$ that is applicable in $s$, there is an abstract action $\bar{a}$ in
$A_F$ that is applicable in $\bar{s}$ and that has the \emph{same qualitative
effects over the features as $a$.}

The actions $a$ and $\bar{a}$ \emph{have the same qualitative effects} in
a state $s$ when they have the same effects on the boolean features $p$,
and the same \emph{qualitative effects} on the numerical features $n$; i.e.,
$a$ decreases (resp.\ increases) the value of the function $\phi_n(s)$ in
$s$ iff $\bar{a}$ decreases (resp.\ increases) the value of the state
variable  $n$.

\begin{example}
  For the features $F=\{H,n(x)\}$, the abstract action $\bar{a}=\abst{\neg H,n(x)>0}{H,n(x)\mminus}$ 
  is sound in the generalized problem $\Q_{clear}$ which consists of all
  Blocksworld instances with stack and unstack actions and goal $clear(x)$.
  Indeed, if $s$ is a reachable state in an instance $P$ and $\bar{a}$ is
  applicable in $\bar{s}$, the gripper must be empty in $s$ and some
  block must be above $x$. Thus, if $A$ is the clear block above $x$ and
  $on(A,B)$ is true in $s$, the action $a=unstack(A,B)$ is applicable in
  $s$ and has the same qualitative effects of $\bar{a}$ over $F$: they
  both make $H$ true and they both decrease the value of $n(x)$.
  Likewise, the abstract action $\bar{a'}=\abst{H}{\neg H}$ is also sound. \qed
\end{example}

For a set of abstract actions to be complete in this example, it must
contain actions to represent all concrete actions in the problem
such as stacking a block above $x$, picking up a block that is
not above $x$, and so on.
It may well be possible that a particular feature set $F$ cannot support
a set of abstract actions $A_F$ that is both sound and complete relative
to $\Q$. Soundness, however, is crucial for deriving general plans as it
ensures that in any state of any instance of $\Q$, an applicable abstract
action  can be ``instantiated'' by one or more concrete actions that have
the same qualitative effects on the features. 


\subsection{Solutions}

A solution to a generalized problem $\Q$ given the features $F$ and abstract
actions $A_F$ is a partial function $\pi$ that maps \emph{abstract states}
into \emph{abstract actions} such that $\pi$ solves all instances $P$ in $\Q$.
The plan or policy $\pi$ induces a trajectory $s_0,a_0,s_1, \ldots, s_n$ in
an instance $P$ of $\Q$ iff
1)~$s_0$ is the initial state in $P$,
2)~$a_i$ is one of the actions that instantiate $\pi(\bar{s}_i)$ in $s_i$,
3)~$a_i$ is applicable in $s_i$ and $s_{i+1}=f(a_i,s_i)$, and
4)~$s_n$ is a goal state of $P$, $n=\infty$, or $\pi(\bar{s}_n)$ is either
undefined, denotes an abstract action that is not applicable in $\bar{s_n}$,
or no applicable action $a_n$ instantiates $\pi(\bar{s}_n)$.
The policy $\pi$ solves $P$ iff all the induced trajectories reach the
goal of $P$.

\begin{example}
  A solution for $\Q_{clear}$ with feature set
  $F=\{H,n(x)\}$ is the policy $\pi$ given by the rules
  $\Rule{\neg H, n(x)>0}{\bar{a}}$ and $\Rule{H, n(x)>0}{\bar{a}'}$
  for the abstract actions $\bar{a}$ and $\bar{a'}$ above.
  The policy picks blocks above $x$ and puts them aside (not above $x$)
  until $n(x)$ becomes zero. \qed
\end{example}


\subsection{Computation}

The steps for obtaining policies $\pi$ for a generalized problem $\Q$
using the features $F$ and abstract actions $A_F$ are as follows \cite{bonet:ijcai2018}:

\begin{enumerate}[1.]
  \item The state variables $V_F$ and the abstract actions $A_F$ are
    extended with initial and goal formulas $I_F$ and $G_F$ over $V_F$
    to yield a \emph{numerical and non-deterministic planning problem}
    $Q_F=\tup{V_F,I_F,G_F,A_F}$. For soundness, $I_F$ must be such that
    the initial states of instances $P$ in $\Q$ all satisfy $I_F$, while
    all states that satisfy $G_F$ must be goal states of $P$.
    % \footnote{A   state $s$ is said to satisfy $I_F$ or $G_F$ if  $\bar{s}$ does.}
    % no longer needed
%
  \item The problem $Q_F=\tup{V_F,I_F,G_F,A_F}$ is converted into a
    \emph{boolean FOND} problem $Q'_F=\tup{V'_F,I'_F,G'_F,A'_F}$ by
    replacing the numerical variables $n\in N$ by the symbols $n=0$,
    the first-order literals $n=0$ by propositional literals  $n=0$, 
    the effects $n\pplus$ by deterministic effects $n > 0$, and the
    effects $n\mminus$ by non-deterministic effects $n>0\,|\,n=0$.
%
  \item The solutions computed by a FOND planner on $Q'_F$ are the
    strong cyclic solutions of $Q'_F$ \cite{strong-cyclic}.
    Such solutions however do not necessarily solve $Q_F$ because the
    non-deterministic effects $n>0\,|\,n=0$ in $Q'_F$ are not \emph{fair} but
    \emph{conditionally fair}: infinite decrements of $n$ ensure
    $n=0$ eventually \emph{only} when the number of increments of $n$ is finite.
    The problem of obtaining solutions of $Q'_F$ that only assume conditional
    fairness, the so-called terminating solutions in \cite{srivastava:aaai2011},
    is mapped into the problem of solving an amended FOND $Q^+_F$ that
     assumes standard  fairness \cite{bonet:ijcai2017}.\footnote{The
    translation to $Q^+_F$ is performed with software supplied by the authors.}
    %available by the authors that fixes bugs in the description found
    %in the paper at https://github.com/bonetblai/qnp2fond.}
\end{enumerate}

%The strong cyclic solutions of  $Q^+_F$ computed by FOND planners off-the-shelf   are solutions to the generalized problem $\Q_F$. 

\begin{example}
  Let restrict  $\Q_{clear}$  to those  instances where  the gripper is initially empty
  and there are blocks on top of $x$. For $F=\{H,n(x)\}$  and $A_F=\{\bar{a},\bar{a'}\}$ as above, 
  $Q_F=\tup{V_F,I_F,G_F,A_F}$ can be defined with $I_F=\{\neg H, n(x) > 0\}$ and $G_F=\{n(x)=0\}$.
  $Q'_F$ is like $Q_F$ but with $n(x)=0$ regarded as a propositional symbol, $n(x) > 0$
  as its negation, and the effect $n(x)\mminus$ of  $\bar{a}$ replaced by  $\neg n(x)=0 \, | \, n(x)=0$.
  Since  no action in $A_F$  increases $n(x)$,   $Q^+_F=Q'_F$, and the strong cyclic solution  of $Q'_F$,
  that corresponds to the policy above, solves $\Q_{clear}$. \qed
\end{example}


\section{Approximate Soundness and Completeness}

Bonet and Geffner's formulation and computational model rest on 
features $F$ and  sound abstract actions $A_F$ provided by hand.
The contribution of this work is to learn them  from samples:


\begin{definition}
  For a generalized problem $\Q$, a \emph{sample set}  $\S$ is a non-empty set of tuples  $(s,s',P)$
  such that 1)~$P$ is an instance of $\Q$, 2)~$s$ is a reachable state in $P$, 3)~$s'$
  is a possible successor of $s$ in $P$; i.e., $s'=f(a,s)$ for some action $a$ applicable in $s$,
  4)~$\S$ is closed in the sense  that if $(s,s',P)$ is in $\S$, then $(s,s'',P)$ is in $\S$  if $s''$ is another
  successor of $s$ in $P$.
\end{definition}

By assuming that sampled states $s$ are tagged with the instance $P$, 
we abbreviate the tuples $(s,s',P)$ as $(s,s')$, and refer to the
states $s'$ in  the pairs $(s,s') \in \S$ as the successors of
$s$ in $\S$. The  closure condition 4)~requires that
states $s$ appearing \emph{first} in pairs $(s,s')$ must be fully expanded in $\S$;
namely, all possible transitions $(s,s'')$ must be in the sample as well. We call them
the \emph{expanded} states in $\S$.

The features and abstract actions for $\Q$ are  learned by
enforcing an approximate form of soundness and completeness
over the samples. Like before,  we take a transition $(s,s')$ and an abstract action $\bar{a}$
to have  the same qualitative effects over a boolean  feature  iff they both make it true , they both make it false,  or
they both leave it unchanged,  and over a numerical feature  iff they both decrease it,  they both increase it,  or they both leave it unchanged.

\begin{definition}
  A set of abstract actions $A_F$ is sound \emph{relative to a  sample set} $\S$ for $\Q$
  iff for  any  abstract action $\bar{a}$ in $A_F$ that is applicable in  a expanded state $s$ of $\S$,
  there is a transition $(s,s')$ in $\S$ that has the same qualitative  effects over $F$ as $\bar{a}$.
\end{definition}

\begin{definition}
  A set of abstract actions $A_F$ is complete   \emph{relative to a sample set} $\S$ for $\Q$
  iff for each transition  $(s,s')$ in  $\S$, there is an abstract action $\bar{a}$ in $A_F$
  with the same qualitative effects over in $F$ that is applicable in $s$.
\end{definition}

For a sufficiently large sample set, these approximate notions converge to their exact versions.

\section{Learning Features and Abstract Actions}

For computing the features and abstract actions we define a formula
$T(\S,\F)$ where ${\cal S}$ is a sample set for $\Q$ and $\cal F$
is a feature pool defined from the primitive domain predicates
as described below. The formula is  \emph{satisfiable} iff there is a set of
features $F \subseteq \F$ and a set of abstract actions $A_F$
over $F$ that is sound and complete relative to the sample set
$\S$.


\subsection{SAT Encoding}

For each transition  $(s, s') \in \S$ and each feature $f \in \F$,
$\Delta_f(s, s') \in \{+, -, \uparrow, \downarrow, \bot\}$ will denote the
\emph{qualitative change of value} of feature $f$ along the transition $(s, s')$,
which can go from false to true ($+$), from true to false ($-$) or remain
unchanged ($\bot$), for boolean features, and can increase ($\uparrow$),
decrease ($\downarrow$), or remain unchanged ($\bot$), for numerical features.
The \emph{propositional variables} in $T(\S,\F)$ are then:
\begin{enumerate}[{\small$\bullet$}]
  \item $selected(f)$ for each $f \in \F$, true iff $f$ selected (in $F$).
  \item $D_1(s,t)$ for states $s$ and $t$ expanded in $\S$,  true iff
    selected features distinguish $s$ from $t$; i.e.\ if $s$ and $t$ disagree
    on the truth value of a selected boolean feature $p$ or atom $n=0$ for a
    selected numerical feature $n$.
    %Since $D_1$ is symmetric, only symbols for $s < t$ are created for some
    %arbitrary, fixed order over the states in $\S_1$.
  \item $D_2(s, s', t, t')$ for each $(s, s')$ and $(t, t')$ in $\S$,
    true iff selected features $f$ distinguish the two transitions; i.e., if
    $\Delta_f(s, s')\not=\Delta_f(t,t')$. 
    %As for $D_1$, the number of $D_2$ variables is cut in half due to symmetries.
\end{enumerate}

\medskip
\noindent The first formulas in $T(\S,\F)$ capture the meaning of $D_1$:
%  
\begin{equation}
  \label{eq:d1}
  D_1(s, t) \ \Leftrightarrow\ \textstyle \bigvee_{f}  selected(f)
\end{equation}

\noindent where $f$ ranges over the features in $\F$ with  different qualitative
values in $s$ and $t$; i.e.\ boolean features $p$ with different truth value
in $s$ and $t$, and numerical features $n$ with different truth values for
 $n=0$ in $s$ and $t$. 

The second formulas encode the meaning of $D_2$:
%
\begin{equation}
  \label{eq:d2}
  D_2(s, s', t, t') \ \Leftrightarrow\ \textstyle\bigvee_f  selected(f)
\end{equation}

\noindent where $f$ ranges over the features $f \in \F$ that have the same
qualitative values in $s$ and $t$ but which change differently in the two
transitions; i.e., $\Delta_f(s, s') \neq \Delta_f(t, t')$.

The third class of formulas relate $D_1$ and $D_2$ by enforcing \emph{soundness}
and \emph{completeness} over  the samples.  Since the abstract actions  are not given,
the qualitative changes in each  transition $(s,s')$ in $\S$
are  taken as templates  of abstract actions. Thus, if $s$ and $t$ are not distinguished by the selected features, 
for each transition $(s, s') \in \S$, there must be a transition $(t, t') \in \S$
such that the two transitions are not distinguished by the selected features either.
This is expressed as
%
\begin{equation}
  \label{eq:bridge1}
  \neg D_1(s, t) \  \Rightarrow\ \textstyle\bigvee_{t'} \neg D_2(s, s', t, t')
\end{equation}

\noindent where $s$ and $t$ are expanded states in  $\S$, $s'$ is a successor of $s$ in $\S$,
and $t'$ ranges over the successors of $t$ in $\S$.

\Omit{ % left out: this is for efficiency only, and is not strictly needed. Also, doesn't fit as such with goal-marking
  Refer to this as an ``efficiency optimization'' ..
  
  such that $s < t$, and transition $(s,s')$ in $\S$. Due to the symmetry breaking caused by the constraint $s < t$,
  the following formulas are also needed

\begin{equation}
  \label{eq:bridge2}
  \neg D_1(s, t) \, \rightarrow \, \bigvee_{s'} \neg D_2(s, s', t, t')
\end{equation}

\noindent where the  iteration in the right hand disjunction goes over the different successor states $s'$ of the first argument
$s$ of $D_1$ that are in the sample set $\S$, $s$ and $t$ are states in $\S_1$ as before with $s < t$, and $t'$ is a
successor state of $t$ in $\S$; i.e. $(t,t') \in \S$.
}
%% --

The last set of clauses force the selected features to distinguish goal
from non-goal states as:
\begin{equation}
  \label{eq:goal}
  D_1(s,t) 
\end{equation}

\noindent where  $s$ and $t$  are expanded states  $\S$ such that exactly one of them is a goal state. 
For this, it is assumed that expanded states $s$ in $\S$ are labeled as goal or non-goal states.

The theory $T(\S,\F)$ given by formulas \eqref{eq:d1}--\eqref{eq:goal}
has a number of atoms that is linear in the size $|{\cal F}|$  of the feature pool $\F$,
and quadratic in the number $m_1$ of expanded states $s$  and
in the number $m_2$ of transitions $(s,s')$ in $\S$.
The number of clauses grows with
$m_1^2 \times (\abs{{\cal F}} + m_2)$, and 
can be roughly cut in half by eliminating symmetries.

We also consider an alternative theory $T_G(\S,\F)$
that trades off completeness guarantees for a much smaller theory size, which
results from not having to consider
all pairs of expanded states or transitions in $\S$.
%
% No need to mark states as well - we just mark transitions, and states are marked implicitly
%
For this, some of the transitions $(s,s')$ in $\S$  are marked as being \emph{goal-relevant},
and completeness is sought \emph{only with respect to these transitions}.
These goal-relevant transitions are selected for being part of a concrete plan
that is computed for each instance $P$ in $\Q$. The theory  $T_G(\S,\F)$ is then as
 $T(\S,\F)$, with the sole difference that the $s$ and $s'$ arguments in the $D_1$ and $D_2$ atoms 
need to be part of some goal-relevant transition.
% This is achieved by
% computing some plans for some instances $P$ in $\Q$, and by adding the
% resulting state transitions to $\S$ at the same time that the first
% arguments $s$ and $(s,s')$ in the $D_1$ and $D_2$ atoms are restricted
% to be marked.
If the set of marked transitions is much smaller than the full sample set, %as will typically be the case,
$T_G(\S,\F)$ is much smaller than $T(\S,\F)$.
$T_G(\S,\F)$ guarantees soundness relative to $\S$
but completeness only relative to the the subset of goal-relevant transitions in $\S$.
Yet this does not affect the resulting formal guarantees, which depend only on soundness. 


\subsection{Extracting $F$ and  $A_F$}

For a satisfying assignment $\sigma$ of the theories $T(\S,\F)$ and $T_G(\S,\F)$, let
$F_{\sigma}$ be the set of features $f$ in $\F$ such that $selected(f)$
is true in $\sigma$, and let $A_{\sigma}$ be the set of abstract actions
that \emph{capture} all transitions $(s,s')$ in $\S$, in the case of theory $T$,
and only the goal-relevant transitions $(s,s')$ in $\S$, in the case of $T_G$.
% with duplicates and redundancies removed.
The abstract action $\bar{a}=\abst{Pre}{\Eff}$ that captures the transition $(s,s')$
has the \emph{precondition} $p$ (resp.\ $\neg p$) if $p$ is a boolean feature in
$F_{\sigma}$ that is true (resp.\ false) in $s$, and has the precondition $n=0$
(resp.\ $n > 0$) if $n$ is a numerical feature in $F_{\sigma}$ such that $n=0$
is true (resp.\ false) in $s$. Similarly, $\bar{a}$ has the \emph{effect} $p$
(resp.\ $\neg p$) if $p$ is a boolean feature in $F_{\sigma}$ that is true
(resp.\ false) in $s'$ but false (resp.\ true) in $s$, and the effect $n\mminus$
(resp.\ $n\pplus$) if $n$ is a numerical feature in $F_{\sigma}$ whose values
decreases (resp.\ increases) in the transition from $s$ to $s'$. 
If two abstract actions $\bar{a}$ and $\bar{a}$ differ only in the sign of a
precondition, the precondition can be dropped, removing the duplicates and
iterating until no more simplifications can be done. Their key properties are:

\begin{theorem}
  The theory $T(\S,\F)$ is satisfiable iff there is a set of features $F \subseteq \F$
  and a set of abstract actions $A_F$ over $F$ such that $A_F$ is sound and complete relative to  $\S$.
\end{theorem}

\begin{theorem}
  If $\sigma$ is a satisfying assignment of $T(\S,\F)$, 
  $A_{\sigma}$ is sound and complete relative to $\S$.
\end{theorem}

\begin{theorem}
  If $\sigma$ satisfies  $T_G(\S,\F)$, $A_{\sigma}$  is sound relative to $\S$.
\end{theorem}

\Omit{
%% commented out
The goal expression $G_F$ can be provided explicitly  in terms of the primitive predicates,
but  more generally can be extracted  from the satisfying assignment $\sigma$ as well.
For this, $G_F = G_{\sigma}$ is defined as the DNF formula whose terms 
correspond to the abstract states over the selected features in $F_{\sigma}$ 
that are false in all expanded states in $\S$ marked as non-goals
and true in some expanded goal state. This DNF formula can   be simplified in standard ways. 
Due to  \eqref{eq:d1} and \eqref{eq:goal} that force the selected features in $F_{\sigma}$
to   distinguish goal  from non-goal states, we have that:

\begin{theorem}
For a satisfying assignment $\sigma$  of $T(\S,\F)$, 
$s$ is expanded goal   state in $\S$  iff  $s$ satisfies  $G_F=G_{\sigma}$.
\end{theorem}
}
% 
Different satisfying assignments $\sigma$ give rise to different features $F_{\sigma}$
and abstract actions $A_{\sigma}$. More meaningful features are found by looking for
satisfying assignments $\sigma$ that minimize a cost measure such as $|F_{\sigma}|$
or, more generally, $\sum_{f \in F_{\sigma}} cost(f)$. We achieve this
optimization by casting the problem as a 
% through the use of Min SAT or 
a Weighted Max-SAT problem where clauses resulting from the above theories are taken as \emph{hard} clauses
and additional \emph{soft} unit clauses $\neg selected(f)$ are given weight $cost(f)$, for $f \in \mathcal{F}$.


\section{Feature Pool}

The computation of the feature pool $\F$ used in $T(\S,\F)$
relies on the predicates used to encode the instances in $\Q$.
A simple grammar allows  us to  construct new predicates 
and to define boolean and numerical features in $\F$
from them. 
In order to construct these predicates, we use an off-the-shelf solution:
\emph{description logics} (DL),  also called \emph{concept languages}, which are well suited to this task.
In DL, \emph{concepts} $C$  refer to unary predicates, and \emph{roles} $R$  to binary predicates.
Concept languages have been used before for  generating general policies using purely learning methods  \cite{martin-geffner:generalized,fern:bias}.
\citeay{dl-handbook} give syntax and semantics of description logics.


\subsection{Grammar}

We use a standard  DL grammar given by the following rules:
\begin{enumerate}[{\small$\bullet$}]
  \item $C \ \leftarrow\ \ p, C_u, C_x$: primitive, universal, nominals: denotation of $C_u$ is whole universe, denotation of $C_x$, for parameter $x$, is $\set{x}$.
  \item $C \ \leftarrow\  \neg C, C \sqcap C'$: negation and conjunction.
  \item $C \ \leftarrow\  \exists R.C , \forall R.C$: concepts that use of  roles: first denotes $x$ s.t.\ for some  $y$,  $R(x,y) \land C(y)$.
  Second,  $x$ s.t.\ for all $y$, $R(x,y) \rightarrow C(y)$.
  \item $C \ \leftarrow\  R=R'$: stands for $x$ s.t.\ set of $y$'s  s.t. $R(x,y)$ equal to set of $y$'s s.t. $R'(x,y)$.
  \item $R \leftarrow R_p, R^{-1}, R^*$: primitive, inverse, transitive closure.
%     \item $R \leftarrow R \circ R'$, role composition: $(x,y)$ in $R \circ R'$ if $(x,z)$ in $R$ and $(z,y)$ in $R'$.
%     \item $R \leftarrow R:C$, denotes pairs $(x,y)$ in $R$ s.t. $C(y)$.
 \end{enumerate}

The primitive predicates $p$ used in the grammar must be common to all instances $P$ of the generalized problem $\Q$.
%They will usually be common domain predicates.
Given that parameters $x$ are common as well,
the compositional semantics of DL defines, for any concept $C$ and role $R$ resulting
from these rules and any state $s$ of $P$, a denotation $C^s$ ($R^s$)
\emph{that can be determined from the denotation of the primitive predicates in $s$ alone}.
Below we see examples of derived concepts such as $\exists\, on^* . C_x$, whose denotation
in any Blocksworld state $s$ is the set of blocks that are above $x$ in $s$.
The numerical features $n_C$ are defined by the cardinality of such denotations $C^s$. 


\subsection{Pool}

We define the \emph{complexity} of a concept or role as the minimum
number of the above rules needed to generate it. We denote by $\G^k$
the set of all concepts and roles with complexity no larger than $k$.
We prune from $\G^k$ \emph{redundant} concepts whose denotation
coincides with that of another concept over all sampled states
in $\S$. 
%
The pool of features $\F=\F^k$  is defined from concepts $C \in \G^k$, and contains
the following features $f$ that represent functions $\phi_f(s)$:
\begin{enumerate}[{\small$\bullet$}]
\item For each nullary primitive predicate $p$, a \emph{boolean} feature $b_p$ that is true in $s$ iff $p^s$ is true.

\item For each concept $C$, 
a \emph{boolean} feature $b_C$, if $|C^s| \in \set{0,1}$ for all sampled states $s$, 
and a \emph{numerical} feature $n_C$ otherwise. The value of $b_C$ in $s$ is true iff $|C^s| = 1$;
the value of $n_C$ is $|C^s|$.

\item Numerical features $\textit{dist}(C_1,R{:}C,C_2)$, that represent the smallest
  $n$ such that for $x_1, \ldots, x_n$, $C_1^s(x_1)$, $C_2^s(x_{n})$, and
  $(R{:}C)(x_i,x_{i+1})$ for $i=1, \ldots, n$. 
\end{enumerate}  

The denotation $(R{:}C)^s$ of role $R{:}C$ contains all pairs $(x,y)$ in $R^s$
s.t. $y$ is in $C^s$.
We also prune from this space of features all features whose denotation is either constant or coincident with that of another feature over all states in $\S$.
The measure $cost(f)$ is set to the complexity of $C$ for $n_C$ and $b_C$,
to $0$ for $b_p$ and to the sum of complexities of $C_1$, $R$, $C$, and $C_2$, for $\textit{dist}(C_1,R{:}C,C_2)$.
Only features with cost bounded by $k$ are allowed in $\F$.


\section{Computational Model: Summary}

In summary, the steps for computing general plans are:
\begin{enumerate}[1.]
  \item a sample set $\S$ is computed from a few instances $P$ of a given
    generalized problem $\Q$,
  \item a pool of features $\F$ is obtained from the predicates used in the
    instances $P$, the grammar, a  bound $k$, and the sample $\S$ (used for
    pruning),
  \item a satisfying assignment $\sigma$ of the theory $T(\S,\F)$ or $T_G(\S,\F)$
    that minimizes $\sum_{f \in F_{\sigma}} cost(f)$ is obtained using a
    Weighted-MAX SAT solver,
  \item the features $F$ and abstract actions $A_F$ are extracted from $\sigma$
    as $F_{\sigma}$ and $A_{\sigma}$,
  \item the problem $Q_F=\tup{V_F,I_F,G_F,A_F}$ is defined with initial and goal
    conditions $I_F$ and $G_F$ provided by hand to match $\Q$, 
  \item the FOND problem $Q^+_F$, constructed automatically from $Q_F$, is solved by an off-the-shelf
    FOND planner.
\end{enumerate}

Notice that the policy $\pi$ that results from the last step deals with
propositional symbols that correspond to atoms $p$ and $n=0$ for boolean
and numerical features $p$ and $n$ in $F$.
When this policy is applied to an instance $P$ of $\Q$, however, such features denote state functions
which are determined by the  semantics of the concepts and roles involved
(e.g., $n_C$ stands for feature function $\phi_{n_c}(s)=|C^s|$).

If we write $A \vDash B$ to express that all the states that satisfy $A$, satisfy $B$, 
the formal guarantees can be expressed as:

\begin{theorem}
If the abstract actions $A_F$ that are sound relative to the sample set $\S$  are actually sound,
then a   policy $\pi$ that solves  $Q^+_F$  is guaranteed to solve  all instances $P$ of $\Q$
with initial and goal situations $I$ and $G$ such that $I \vDash  I_F$ and $G_F \vDash G$.
\end{theorem}

This means that a policy $\pi$ obtained with a FOND planner from $Q^+_F$ 
solves the generalized problem $\Q$, provided that the abstract actions
learned are sound relative to $\Q$ and that $\Q$ is restricted to the instances $P$ whose
initial and goal conditions comply with   $I_F$ and $G_F$.
However,  for a policy to be correct, the abstract actions do
not have to be sound relative to all reachable states $s$ in the instances $P$ of $\Q$, it  suffices
if they are sound on the states $s$ that are reachable with the policy. We see an example of this below.


% \textcolor{red}{\bf ** NEXT IS UNCLEAR **}
% ok, I removed this. It's not strictly needed and we don't use it
\Omit{
Since we do not use a language for specifying $\Q$ formally, we will 
take the formulas $I_F$ and $G_F$  as a partial formal specification of
$\Q$. The theorem then says that if soundness over the samples, ensure
formal soundness over $\Q$, the policy that solves the FONDP problem $Q^+_F$,
solves $\Q$. The result holds whether $T$ or $T_G$ (goal marking) is used in step 3.
In principle, we could learn the formulas $I_F$ and $G_F$ from the sample as well,  but with different guarantees. 
}


\section{Experimental Results}

We evaluate the computational model on four generalized problems $\Q$. For each $\Q$, we have selected  a few and small  ``training''  instances $P$ in $\Q$ by hand, 
from which we draw  the sample sets $\cal S$.  $\S$ is constructed by collecting  the first $m$ states
generated by  a breadth-first search (where $m$ is typically never larger than a few hundreds)
along with  the states generated in an \emph{optimal} plan. This is done for each
training instance  $P$. The plans ensure that $\cal S$ contains some goal states, and they are used to mark states and transitions for
constructing  the theory $T_G({\cal S,\cal F})$, which is the one used in the experiments. $\S$ is closed by fully expanding the states selected
while the bound $k$ for generating  $\cal F$ is set to $8$. The  distance features are used only in the last problem.
The Weighted-Max Solver is Open-WBO \cite{martins2014open} and the FOND planner is SAT-FOND \cite{tomas:fond-sat}.
The translation from $Q'_F$ to $Q^+_F$ is very fast, in  the order of $0.01$ seconds in all cases.  
The whole computational pipeline summarized by the steps 1--6 above is processed on Intel Xeon E5-2660 CPUs
with time and memory cutoffs of 1h and 32GB. Table~\ref{tab:exp-result-data} summarizes the relevant data for 
the problems. 




\begin{table*}[t]
  \centering
  %\resizebox{.95\textwidth}{!}{
    \begin{tabular}{lrrrrr@{}rrrrrrrr}
      \toprule
                     &     &            &            & \multicolumn{2}{c}{$T(\S,\F)$} && \multicolumn{2}{c}{$T_G(\S,\F)$} \\
      \cmidrule{5-6}
      \cmidrule{8-9}
                     & $n$ & $\abs{\S}$ & $\abs{\F}$ &   $np$ &   $nc$ &&   $np$ &   $nc$ & $t_{\text{SAT}}$ & $\abs{F}$ & $\abs{A_F}$ & $t_{\text{FOND}}$ & $\abs{\pi}$ \\
      \midrule
      $\Q_{clear}$   &   1 &        927 &        322 &   535K &  59.6M &&   7.7K &   767K &             0.01 &         3 &           2 &              0.46 &           5 \\
      $\Q_{on}$      &   3 &        420 &        657 &   128K &  25.8M &&  18.3K &   3.3M &             0.02 &         5 &           7 &              7.56 &          12 \\
      $\Q_{gripper}$ &   2 &        403 &        130 &    93K &   4.7M &&   8.1K &   358K &             0.01 &         4 &           5 &            171.92 &          14 \\
      $\Q_{reward}$  &   2 &        568 &        280 &   184K &  11.9M &&  15.9K &   1.2M &             0.01 &         2 &           2 &              1.36 &           7 \\
      \bottomrule
    \end{tabular}
  %}
  \caption{Results on 4 different benchmarks: $n$ is number of training instances $P$,
    $\abs{\S}$ is number of transitions in $\S$,
    %$k$ is max.\ feature complexity,
    $\abs{\F}$ is size of feature pool,
    $np$ and $nc$ are the number of propositions and clauses in theories $T(\S,\F)$ and $T_G(\S,\F)$,
    $t_{\text{SAT}}$ is time for SAT solver on theory $T_G$, 
    $\abs{F}$ and  $\abs{A_F}$ are the resulting number of 
    selected features and abstract actions, 
    $t_{\text{FOND}}$ is time for the FOND planner, and
    $\abs{\pi}$ is the size of the resulting policy.
    %$time_{\text{SAT}}$, $T_{\text{FOND}}$: computation time (sec.) of the SAT and FOND solvers, resp.;
    %$T_{\text{trans}}$: time (sec.) to generate the FOND problem $Q^+_F$;
    %$\#Act$, $\#At$: number if actions and atoms in $Q^+_F$;
    All times are in seconds. %figures that depend on the SAT theory are given for $T_G(\S,\F)$.
  }
  \label{tab:exp-result-data}
\end{table*}


% For each problem: 1--6 details that vary from problem to problem. Follow same structure:
% $\Q$, $P$s, primitive predicates, number of atoms and clauses  of $T$ and/or $T^G$,
% SAT times,  $F$, $|A_F|$;  number of FOND atoms and actions in $Q^+_F$ (include those in $F$, $A_F$ plus translation), $I_F$ and $G_F$,
% FOND time, resulting policy.  Numbers can go all in a table.


\paragraph{Clearing a block.}
$\Q_{clear}$ contains the Blocksworld instances  with goals of the form  $clear(x)$
and stack/unstack actions. The primitive predicates, i.e., those appearing in all instances $P$ of $\Q_{clear}$, 
are $on(\cdot,\cdot)$, $clear(\cdot)$,
$ontable(\cdot)$, $holding(\cdot)$, and $handempty$. 
For this problem, a  single training instance $P$  with  5 blocks
sufficed to learn an abstract model from which a general plan was computed.
The  set of  features $F$ obtained from the theory $T_G$ is:

\begin{enumerate}[--]
  \item $H: holding$ (whether some block is being held),
  \item $X: holding \sqcap C_x$ (whether block $x$ is being held),
  \item $n(x): \abs{\exists\,on^* . C_x}$, (number of blocks above block $x$).
\end{enumerate}

\noindent The set of abstract actions obtained from $T_G$ is:\footnote{
Feature and action names are provided   to make their intuitive meaning explicit.
Their formal meaning is given by their definitions.}

\begin{enumerate}[--]
  \item $\text{put-aside} = \abst{\neg X, H}{\neg H}$,
  \item $\text{pick-above-$x$} = \abst{\neg X, \neg H, n(x) > 0}{H, n(x)\mminus}$.
\end{enumerate}

The extra precondition $\neg X$ is not needed and would  be removed by drawing more samples,
yet it does not prevent the computation of a solution to $\Q_{clear}$ using  $I_F = \{\neg H, \neg X, n(x) > 0\}$ and $G_F=\{n(x)=0\}$.
The translator produces a FOND problem in 0.01 seconds that is solved
by the FOND  planner in 0.46 seconds. The resulting plan implements  a loop that applies
the pick-above-$x$ action followed by put-aside,  until $x$ becomes clear. 


% Action 1: PLACE-ASIDE
%         PRE: NOT holding(a), bool[holding]
%         EFFS: NOT bool[holding]
% 
% Action 2: PICK-FROM-A
%         PRE: NOT bool[holding], NOT holding(a), n(a) > 0
%         EFFS: DEC n(a), bool[holding]

% Loop:
%   Action 2 [Pick block above x]
%   If n(a) = 0 Break
%   Action 1 [Put block aside]
%

\paragraph{Stacking two blocks.}
$\Q_{on}$ consists  of Blocksworld instances with goals of the form $on(x,y)$,
and initial situations where the blocks $x$ and $y$ are not in the same tower.
The primitive predicates are the same as in $\Q_{clear}$.
For this problem, three training instances $P$ were used.
The set $F$ obtained from $T_G$ contains
% 
% Using the above Blocksworld encoding and only 3 different training
% instances with up to 14 blocks, an abstract model involving 3 boolean
% features, 2 numerical features, and 7 actions is learnt.
the boolean features  $E$, $X$ and $G$,  that stand for the gripper
being empty, the gripper holding block $x$, and the block $x$ being
on block $y$, and the numerical features  $n(x)$ and $n(y)$ that stand
for the number of blocks above $x$ and $y$ respectively.
The DL  definition of the  features $X$, $n(x)$, and $n(y)$ are as above
while $E$ is $handempty$ and $G$ stands for $(\exists\,on.C_y)  \sqcap C_x$.
% Interestingly, this last feature is the concept-based representation
% of the goal $on(x,y)$.
%
%% blocks
%% 5 bool[handempty] 0 n(a) 1 n(b) 1 holding(a) 0 on(a,b)_2 0
%% 5 bool[handempty] 1 holding(a) 0 n(a) 1 n(b) 1 on(a,b)_2 0
%% 4 holding(a) 0 n(a) 0 n(b) 1 on(a,b)_2 1
%% 7
%% action_1 ; put held block (diff. from a and b) aside
%% 4 bool[handempty] 0 holding(a) 0 n(a) 0 on(a,b)_2 0
%% 1 bool[handempty] 1
%% action_2 ; put aside
%% 5 bool[handempty] 0 holding(a) 0 n(a) 1 n(b) 1 on(a,b)_2 0
%% 1 bool[handempty] 1
%% action_3 ; pick block a
%% 5 bool[handempty] 1 holding(a) 0 n(a) 0 n(b) 0 on(a,b)_2 0
%% 2 bool[handempty] 0 holding(a) 1
%% action_4 ; put a (being held) aside
%% 5 bool[handempty] 0 holding(a) 1 n(a) 0 n(b) 1 on(a,b)_2 0
%% 2 bool[handempty] 1 holding(a) 0
%% action_5 ; pick block different from b that is above a
%% 5 bool[handempty] 1 holding(a) 0 n(a) 1 n(b) 1 on(a,b)_2 0
%% 2 bool[handempty] 0 n(a) 0
%% action_6 ; pick block different from a that is above b
%% 5 bool[handempty] 1 holding(a) 0 n(a) 0 n(b) 1 on(a,b)_2 0
%% 2 bool[handempty] 0 n(b) 0
%% action_7 ; put a directly on b
%% 5 bool[handempty] 0 holding(a) 1 n(a) 0 n(b) 0 on(a,b)_2 0
%% 4 bool[handempty] 1 holding(a) 0 n(b) 1 on(a,b)_2 1
The learned set $A_F$ includes actions to pick up the clear
block above $x$ or $y$, two actions to put aside the block being held,
and actions to pick block $x$, to place $x$ on $y$, and to place
$x$ on the table:
\begin{enumerate}[--]
  \item $\text{pick-ab-$x$}\!=\!\abst{E,\neg X,\neg G,n(x)\!\!>\!0,n(y)\!>\!0}{\neg E,n(x)\mminus}$,
  \item $\text{pick-ab-$y$}\!=\!\abst{E,\neg X,\neg G,n(x)\!=\!0,n(y)\!\!>\!0}{\neg E,n(y)\mminus}$,
  \item $\text{put-aside-1}=\abst{\neg E,\neg X,\neg G,n(x)=0}{E}$,
  \item $\text{put-aside-2}=\abst{\neg E,\neg X,\neg G,n(x)>0,n(y)>0}{E}$,
  \item $\text{pick-$x$}=\abst{E,\neg X,\neg G,n(x)=0,n(y)=0}{\neg E,X}$,
  \item $\text{put-$x$-aside}\!=\!\abst{\neg E,X,\neg G,n(x)=0,n(y)>0}{E,\neg X}$,
  \item $\text{put-$x$-on-$y$} = \langle \neg E, X, \neg G, n(x)=0, n(y)=0; E, \neg X,$ $G, n(y)\pplus \rangle$.
\end{enumerate}

The resulting abstraction with $I_F\!=\!\{ E, \neg X, \neg G, n(x)\!>\!0, n(y)\!>\!0\}$
and $G_F = \{ \neg X, G, n(x)=0, n(y)\!>\!0\}$, is translated into the FOND problem
$Q_F^+$  in 0.01 seconds,
which is solved by the FOND planner  in 7.56 seconds.
The policy implements a loop  to clear block $x$ as above, 
followed by a loop to clear block $y$, and finishes by picking
$x$ and placing it on $y$. 


\paragraph{Gripper.}
$\Q_{gripper}$ contains classical Gripper instances  where a robot with
some grippers needs to move a number of balls from one room into another
\emph{target} room $x$. Each gripper may carry one ball
at a time.
The standard STRIPS predicates are $at\text{-}robby(l)$, $at\text{-}ball(b,l)$,
$free(g)$, $carry(b,g)$ that denote, respectively, whether the robot
(the ball) is at location $l$, whether gripper $g$ is free, and whether
gripper $g$ carries ball $b$, plus unary type predicates $room$, $ball$,
and $gripper$. The type predicates are also included. 

Pipeline 1--6 is fed  2 training instances $P$ from $\Q_{gripper}$  with 2
rooms each, one with 4 balls and the other with 5. The set of features that
is obtained from the theory $T_G$ is:
\begin{enumerate}[--]
  \item $X: at\_robby \sqcap C_x$ (whether robby is in target room),
  \item $B: |\exists\,at . \neg C_x|$ (number of balls not in target room),
  \item $C: |\exists\,carry . C_u|$ (number of balls carried),
  \item $G: \abs{free}$ (number of empty grippers).
\end{enumerate}

\noindent The resulting abstract actions are:

\begin{enumerate}[--]
  \item $\text{drop-ball-at-$x$}=\abst{C>0, X}{C\mminus, G \pplus}$,
  \item $\text{move-to-$x$-half-loaded}\! =\!  \abst{\neg X, B=0, C>0, G\!>\!0}{X}$,
  \item $\text{move-to-$x$-fully-loaded} = \abst{\neg X, C>0, G=0}{X}$,
  \item $\text{pick-ball-not-in-$x$} = \abst{\neg X, B > 0, G > 0}{B\mminus, G\mminus, C\pplus}$,
  \item $\text{leave-$x$} = \abst{X, C=0, G > 0}{\neg X}$.
\end{enumerate}

Once again, although this set of abstract actions  is not complete,
it is sufficient for generating a general plan. The abstraction with
$I_F = \{ B>0, \neg X, G>0, C=0\}$ and $G_F = \{ B=0, G>0, C=0 \}$ is
translated into the FOND problem $Q^+_F$, which is solved by the
planner in 171.92 seconds. 
The resulting policy picks the balls in the source room $y$, one by one, 
until the grippers are full or there are no more balls in $y$.
It then moves to room $x$, with one of the two move actions above
according to whether the grippers are fully loaded or not,
and then drops the balls in room $x$ one by one, until the grippers
become all empty. It then moves to room $y$ again if there are balls
still to be moved, and the process repeats. Like before, the extra conditions
that are not strictly required (in the move actions), do not preclude
the solution of the problem, that results in a general plan that 
works  for robots with any  number of grippers and  any number of balls. 


\paragraph{Collecting rewards.}
$\Q_{reward}$ contains instances where  an agent needs to navigate a rectangular
grid to pick up  rewards spread in a grid  while 
avoiding \emph{blocked} cells. A variation of the example
has been used recently in \cite{garnelo2016towards}.
The STRIPS instances have the primitive predicates $reward(\cdot)$, $at(\cdot)$, $blocked(\cdot)$
and $adjacent(\cdot,\cdot)$ that denote the position of the rewards, of
the agent, of the blocked cells, and the grid topology respectively.
Pipeline 1--6  is fed with two instances of the problem of sizes $4 \times 4$
and $5\times 5$, and two different  given distributions of blocked cells  and rewards.
From the theory $T_G$, two numerical  features are learned:
\begin{enumerate}[--]
  \item $R: \abs{reward}$ (number of remaining rewards), 
  \item $D: dist(at, adjacent{:}(\neg blocked), reward)$ (distance from current
    cell to closest cell with reward, traversing adjacent, unblocked cells only).
\end{enumerate}
Likewise, two abstract actions are obtained:
\begin{enumerate}[--]
  \item $\text{collect-reward} = \abst{D=0, R > 0}{R\mminus, D\pplus}$,
  \item $\text{move-to-closest-reward} = \abst{R>0, D>0}{D\mminus}$.
\end{enumerate}

The abstraction with $I_F = \{ R>0, D>0 \}$ and $G_F=\{R=0,D>0\}$
is translated into the  FOND problem $Q^+_F$ which yields
a policy  in 1.36 seconds. The policy moves the agent
one step at a time, towards  the uncollected  reward
that is closest, as measured by the numerical feature $D$.
Once the reward is reached, the reward is consumed, and
the process repeats until there are no more rewards to be collected. 


\subsection{Observations}

We conclude the section with a few observations.

\paragraph{Soundness.} The abstract action \text{pick-ab-$y$} in $\Q_{on}$
is sound relative to the sample set $\S$ used  but is not sound relative
to $\Q_{on}$. In any state $s$ where $x$ is a few blocks above $y$ and clear,
there is no concrete action that instantiates \text{pick-ab-$y$},
as picking up the topmost block above $y$ would necessarily make feature $X$ (holding block $x$)
true, which the abstract action does not. Still, the resulting policy is correct
for $\Q_{on}$ because such states $s$ are not  visited by the policy
from the given initial conditions.


\paragraph{Expressivity.} The general DL grammar that we have described
is adequate for many  domains but may fall short  on others. 
For example,  in Towers of Hanoi, the first action to be taken
(whether to move the smallest disk to target or buffer peg) depends
on the number of disks being even or  odd, which is not a feature
that  would result  from the primitive predicates used in the STRIPS
instances and the general grammar. Yet, for some reason,
Tower of Hanoi is regarded as a puzzle.  For other problems, the  use of \emph{goal predicates};
namely, copies of some primitive predicates  evaluated in the goal
(e.g., for solving arbitrary Blocksworld instances) would be  necessary  \cite{martin-geffner:generalized}.


\paragraph{Scalability.} The current bottleneck of the approach is 
in the size of the SAT  theories used to derive the features
and the actions. This puts a limit on the complexity of the
feature grammars, the bound $k$, and the number of state
transitions in the sample, as they  result otherwise
in SAT theories that are just too large (see Table~\ref{tab:exp-result-data}).
This is indeed the reason why have chosen to use the theories $T_G$
with goal marking. Additional ideas, however, are required
to improve scalability and to make the computational model
captured by  steps 1--6 more robust.


\section{Discussion}

We have introduced a scheme for computing general plans that mixes
learning and planning: a \emph{learner} infers a general abstraction made
up of features and abstract actions by enforcing a notion of soundness
and completeness over a  set of samples, and a \emph{planner}
uses the abstraction, suitably transformed, to compute general plans. 
The number of samples required for obtaining correct general plans is
very small as the learner does not have to learn how to plan, but
just identify the features that are relevant for the  planner to track.
Moreover, unlike purely learning approaches, the scope and correctness
of the resulting general plans can be assessed.
We have carried out a number of experiments over several domains to 
illustrate the versatility of the approach. In the future, we want 
to study the formal characterization  of the generalized problem $\Q$ and the possibilities of
testing soundness over $\Q$ automatically, and to improve the scalability of the approach,
for example, by collecting the samples intelligently and incrementally
from the states where the computed policy fails, and by reusing the
features and actions obtained for some goal for more complex goals.
The learned features may also have other uses.


\bibliographystyle{aaai}
\bibliography{control}

\end{document}

