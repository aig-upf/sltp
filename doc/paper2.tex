\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{booktabs}  %Required

\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required

\setcounter{secnumdepth}{0}  

% \usepackage{soul}
% \usepackage[hidelinks]{hyperref}
% \usepackage[utf8]{inputenc}
% \usepackage[small]{caption}
%\usepackage{helvet}
%\usepackage{graphicx}
%\usepackage{url}

%\usepackage{color,soul}
%\newcommand\sr[1]{\sethlcolor{yellow}\hl{SR: #1}\sethlcolor{cyan}}
%\newcommand\hg[1]{\sethlcolor{green}\hl{HG: #1}\sethlcolor{cyan}}
%\newcommand\gdg[1]{\sethlcolor{yellow}\hl{GDG: #1}\sethlcolor{cyan}}
%\newcommand\bb[1]{\sethlcolor{cyan}\hl{BB: #1}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
%\usepackage{upgreek}
%\usepackage[normalem]{ulem}
\usepackage{multirow}
% \usepackage[small]{caption}


\usepackage{xspace}
% \usepackage{ifthen}
\usepackage{tikz}
% \usetikzlibrary{shapes.geometric}
% \pgfdeclareimage[width=1cm]{dirt}{dirt2}

% \usetikzlibrary{shapes.geometric}
% \pgfdeclareimage[width=1cm]{dirt}{dirt2}

\newcommand{\Omit}[1]{}
\newcommand{\denselist}{\itemsep -1pt\partopsep 0pt}
\newcommand{\tup}[1]{\langle #1 \rangle}
\newcommand{\pair}[1]{\langle #1 \rangle}
\newcommand{\tuple}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\set}[1]{\ensuremath{\left\{#1 \right\}}}
\newcommand{\setst}[2]{\ensuremath{\left\{#1 \mid #2 \right\}}}
\newcommand{\abs}[1]{\ensuremath{\left\vert{#1}\right\vert}}
% \newcommand{\citeay}[1]{\citeauthor{#1} [\citeyear{#1}]}
\newcommand{\citeay}[1]{\citeauthor{#1} (\citeyear{#1})}
\newcommand{\citeaywithnote}[2]{\citeauthor{#1} (\citeyear[#2]{#1})}


\newtheorem{definition}{Definition}
\newtheorem{definitionandtheorem}[definition]{Definition and Theorem}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{example}[definition]{Example}

\def\withproofs{0} % to include proofs in the draft: 0=don't include, 1=include
\newcommand{\nosuchproof}{\textcolor{red}{\bf No such proof yet...}}
\newcommand{\problem}{\textcolor{red}{\bf **** PROBLEM ****}}
\newcommand{\alert}[1]{\textcolor{red}{\bf #1}}
\newcommand{\CHECK}[1]{\textcolor{red}{\bf Note: #1}}

\newcommand{\hobs}{\ensuremath{h_{obs}}\xspace}
\newcommand{\indicator}[1]{[\![#1]\!]}
\newcommand{\C}{\mathcal{C}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\dexp}{\text{2-}\!\exp}

\newcommand{\Pn}{\ensuremath{P_n}\xspace}
\newcommand{\Pnm}{\ensuremath{P_{n,m}}\xspace}
\newcommand{\muwf}{\ensuremath{\mu_{w\!f}}\xspace}

\newcommand{\true}{\mathsf{true}}
\newcommand{\false}{\mathsf{false}}
\newcommand{\Eff}{{\mathit{Eff}}}

%% C++
\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
%%% \def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}
%%% \newcommand{\pplus}{\!\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
%%% \newcommand{\mminus}{\!\hspace{-.05em}\raisebox{.4ex}{\tiny\bf -}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf - }}
%%%\newcommand{\pplus}{\hspace{-.05em}\raisebox{.2ex}{\footnotesize\mit +}\nolinebreak\hspace{-.10em}\raisebox{.2ex}{\footnotesize\mit +}}
%%%\newcommand{\mminus}{\hspace{-.05em}\raisebox{.2ex}{\footnotesize\mit -}\nolinebreak\hspace{-.07em}\raisebox{.2ex}{\footnotesize\mit -}}

% abstract actions and plans
\newcommand{\abst}[2]{\tup{#1;#2}}
\newcommand{\Rule}[2]{\ensuremath{#1 \Rightarrow #2}}

% decrements and increments
\newcommand{\pplus}{\hspace{-.05em}\raisebox{.15ex}{\footnotesize$\uparrow$}}
\newcommand{\mminus}{\hspace{-.05em}\raisebox{.15ex}{\footnotesize$\downarrow$}}

% numerical effects
\newcommand{\dec}[1]{#1:=#1-1}
\newcommand{\inc}[1]{#1:=#1+1}

% qualitative effects
\newcommand{\Dec}{Dec}
\newcommand{\Inc}{Inc}

% STRIPS actions
\newcommand{\Stack}{\text{Stack}}
\newcommand{\Unstack}{\text{Unstack}}
\newcommand{\Pickup}{\text{Pickup}}
\newcommand{\Putdown}{\text{Putdown}}
\newcommand{\Move}{\text{Move}}

% Examples
\newcommand{\Example}{\medskip\noindent\textbf{Example.}\xspace}


%PDF Info Is Required (no accents, etc.!):
\pdfinfo{
/Title ()
/Author ()}


\title{Learning features and abstract actions for computing generalized plans} 

% \author{B. Bonet, G. Frances, H. Geffner} % Alphabetical
% 
\author{\textbf{Submission \#5695}}


\begin{document}


\maketitle

\begin{abstract}
  Generalized planning is concerned with the computation of plans that solve multiple instances.
  Recently, it has been shown that generalized plans can be expressed as mappings of feature
  values into actions, and that they can often be computed with fully observable non-deterministic
  (FOND) planners. %  on suitable FOND problems. %after a sequence of transformations.
  The actions in such plans, however, are not the actions in the instances themselves, which are
  not necessarily common to  other instances, but abstract actions that track the effect of 
  actions on the set of features. The formulation assumes that both the features and the abstract
  actions are given.
  In this work, we address this limitation by showing how to learn them automatically. 
  %  the features and  abstract actions.
  The resulting account of generalized planning combines learning and planning
  in a novel way: a \emph{learner}, based on a min SAT formulation, yields the features and abstract
  actions from state transitions sampled from problem instances, and a FOND \emph{planner}
  uses this information, suitably transformed, to produce the general plans.
  Correctness guarantees are given and experimental results on several domains are reported.
\end{abstract}


\section{Introduction}

Generalized planning studies the computation of plans  that   solve   multiple  instances
\cite{srivastava08learning,bonet09automatic,srivastava:generalized,hu:generalized,BelleL16,anders:generalized}.
For example, the plan  that iteratively  picks  a  clear block above $x$ 
and places it  on the table, achieves the goal $clear(x)$
in \emph{any} instance of the Blocksworld where the gripper is initially empty.
Once this  general plan or  policy is derived it can be  applied to solve an infinite collection
of instances that involve different initial states, different objects, and hence  different (ground)  actions. 


In the basic formulation due  to \citeay{hu:generalized}, a generalized plan
is a mapping of observations into actions that are assumed to be common
among  all the instances. More recently, this formulation has been extended by
\citeay{bonet:ijcai2018} to account for relational domains like Blocksworld  where the set of
objects and  actions change from instance to instance.
In the new formulation, the observations  are replaced by  a set of boolean
and numerical features $F$ and a set of \emph{abstract actions} $A_F$.
The  abstract actions are  \emph{sound} and \emph{complete} if they track the
effects  of the actions on  the features in a suitable way.
The resulting generalized plans  map feature values into abstract actions, and soundness
ensures that the application  of an   abstract action  results in the application of
a concrete action with a similar  effect  on  the features. Moreover, the form of the abstract actions
ensures that generalized plans can be computed using fully observable non-deterministic (FOND) planners
once the generalized planning problem is transformed into a FOND problem.
%after some transformations.

\Omit{
Abstract actions can increase or decrease numerical variables  $n$
associated with the numerical features (e.g., number of blocks above $x$), and while
increments  are transformed into deterministic propositional  effects $n > 0$ where $n > 0$ is
the negation of the proposition $n=0$, decrements  are  transformed  into
non-deterministic (disjunctive)  propositional effects $n > 0 \, | \, n=0$. 
}

Bonet and Geffner's  formulation of generalized planning assumes that the
features and abstract actions are given. In this work we address this limitation
by  \emph{showing  how the features and abstract actions  can be learned from
the primitive predicates used to define the instances and a given sample of state transitions.}
For example,  the general policy for achieving $clear(x)$ % of \citeay{bonet:ijcai2018}
is obtained using a FOND planner on an abstraction that consists of  a  boolean
feature $H$  that tracks whether the gripper holds a block,  a numerical feature
$n(x)$ that counts the number of blocks above $x$, and two abstract actions:
one with preconditions $\neg H$ and $n(x) > 0$, and effects $H$ and $n(x)\mminus$
(decrement $n(x)$), and the other with precondition $H$ and effect $\neg H$.
In this work, we show how to obtain such policies from STRIPS instances only
without having to provide the features and abstract actions by hand.

The work has relation to a number of research threads in planning, knowledge
representation, and machine learning. We make use of SAT solvers and description
logics for learning features and abstract actions \cite{sat-handbook,dl-handbook}.
The abstract actions provide a model from which the plans are obtained via
transformations and FOND planners \cite{geffner:book,ghallab:book}.
The model is not the model of an instance but a generalized model for obtaining
plans that work for multiple instances. In this sense, the work is different than
action model learning \cite{yang:action-learning} and model-based reinforcement
learning and closer in aims to work on learning general policies from examples or
experience \cite{martin:concept,fern:bias,mazebase,general-drl}.
Generalized planning has also been formulated as a problem in first-order logic
\cite{srivastava:generalized} and general plans over finite horizons have been
derived using first-order regression \cite{boutilier2001symbolic,wang2008first,van2012solving}.
Our approach differs from first-order approaches in the use of propositional
planners, and from purely learning approaches, in the \emph{formal guarantees} that
are characteristic of planning: if the learned abstract actions are actually sound,
the resulting general plans are correct. 

The paper is organized as follows. We provide the background  first and show then
how features and abstract actions can be learned by enforcing soundness and
completeness over a set of samples  using features derived from the domain predicates.
The full computational model is then summarized, followed by experimental results
and a discussion.


\section{Background}

We review generalized planning, abstract actions, and computation of solutions 
following  \citeay{bonet:ijcai2018}.


\subsection{Generalized Planning}

A \emph{generalized planning problem} $\Q$ is a collection of planning instances $P$.
An instance $P$ is a  classical planning problems expressed in some compact language
as a tuple $P=\tup{V,I,G,A}$ where $V$ is a set of state variables that can take a
finite set of values (boolean or not), $I$ is a set of atoms over $V$ defining an
initial state $s_0$, $G$ is a set of atoms or literals over $V$ describing the goal
states, and $A$ is a set of actions $a$ with their preconditions and effects that
define the set $A(s)$ of actions applicable in any state $s$, and the successor
state function $f(a,s)$, for any state $s$ and $a \in A(s)$.
A state is a valuation over $V$, and a solution to $P$ is an applicable action
sequence $\pi=a_0,\ldots,a_n$ that generates  a state sequence $s_0,s_1,\ldots,s_{n}$
where $s_n$ is a goal state (makes $G$ true). In this sequence, $a_i \in A(s_i)$
and $s_{i+1}=f(a_i,s_i)$ for $i=0, \ldots, n-1$. A state $s$ is reachable in $P$
if $s=s_n$ for one such sequence. A solution to the \emph{generalized problem}
$\Q$ is a solution to all instances $P \in Q$. The form of such solutions is
described below.

A \emph{feature} $f$ for a class $\Q$ of problems represents a function $\phi_f$
that takes an instance $P$ from $\Q$ and a state $s$ reachable in $P$, and results
in a value denoted as $\phi_f(s)$. 
A feature is \emph{boolean} if it results in a boolean value, and \emph{numeric}
if it results in a numerical value, that we assume to be a non-negative integer.
$H$ and $n(x)$ represent two features in the Blocksworld:
the first is a boolean feature that tracks whether the gripper is empty, while
the second is numerical and tracks the number of blocks above $x$. 

Symbols like $x$ denote \emph{parameters} whose value depends on the
instance $P$ in $\Q$. For example, if $\Q_{clear}$ denotes the Blocksworld
instances with goals of the form $clear(x)$, the value of $x$ in an instance
$P$ with goal $clear(a)$ is $a$.


\subsection{Abstract Actions}

An \emph{abstract action}  for a generalized problem $\Q$ and a set of features $F$
is a pair $\bar{a}=\abst{Pre}{\Eff}$ where $Pre$ and $\Eff$ are the  preconditions
and effects expressed in terms of a  set $V_F$ of boolean and numerical \emph{state variables}
associated with the features in $F$. Preconditions and effects over boolean state variables $p$ are literals of the form $p$
and $\neg p$, that abbreviate the  atoms $p=true$ and $p=false$, while  over  numerical state variables $n$,
preconditions are of  the form $n=0$ and $n > 0$, and effects  of the form $n\mminus$ (decrements) and
$n\pplus$ (increments).

Features $f$ refer to \emph{state functions} $\phi_f(s)$ in the instances
of the generalized problem $\Q$, but to \emph{state variables} in the
abstraction. Likewise, an \emph{abstract state} is  a truth valuation over
the state variables $p$ associated with the boolean features and the atoms
$n=0$ over the state variables $n$ associated with the numerical features.
The abstract state $\bar{s}$ that \emph{corresponds} to a concrete state $s$
is the truth valuation that makes $p$ true iff $\phi_p(s)=true$, and $n=0$
true iff $\phi_n(s) = 0$. An abstract action $\bar{a}$ is applicable in
a state $s$ if its preconditions are satisfied in $\bar{s}$.

A set of abstract actions $A_F$ is \emph{sound} relative to a generalized
problem $\Q$ iff for any reachable state $s$ over an instance $P$ in $\Q$
and any abstract action $\bar{a}$ in $A_F$ that is applicable in $\bar{s}$,
there is a concrete action $a$ in $P$ that is applicable in $s$ and that
has the \emph{same qualitative effects as} $\bar{a}$ over the features.
The action $a$ \emph{represents} or \emph{instantiates} the abstract action
$\bar{a}$ on $s$. 
%
Similarly, a set of abstract actions $A_F$ is \emph{complete} iff for any
reachable state $s$ over an instance $P$ in $\Q$ and any concrete action $a$
in $P$ that is applicable in $s$, there is an abstract action $\bar{a}$ in
$A_F$ that is applicable in $\bar{s}$ and that has the \emph{same qualitative
effects over the features as $a$.}

The actions $a$ and $\bar{a}$ \emph{have the same qualitative effects} in
a state $s$ when they have the same effects on the boolean features $p$,
and the same \emph{qualitative effects} on the numerical features $n$; i.e.,
$a$ decreases (resp.\ increases) the value of the function $\phi_n(s)$ in
$s$ iff $\bar{a}$ decreases (resp.\ increases) the value of the state
variable  $n$.

\begin{example}
  For the features $F=\{H,n(x)\}$, the abstract action $\bar{a}=\abst{\neg H,n(x)>0}{H,n(x)\mminus}$ 
  is sound in the generalized problem $\Q_{clear}$ which consists of all
  BW instances with stack and unstack actions with goal $clear(x)$.
  Indeed, if $s$ is a reachable state in an instance $P$ and $\bar{a}$ is
  applicable in $\bar{s}$, the gripper must be empty in $s$ and some
  block must be above $x$. Thus, if $A$ is the clear block above $x$ and
  $on(A,B)$ is true in $s$, the action $a=unstack(A,B)$ is applicable in
  $s$ and has the same qualitative effects of $\bar{a}$ over $F$: they
  both make $H$ true and they both decrease the value of $n(x)$.
  Likewise, the abstract action $\bar{a'}=\abst{H}{\neg H}$ is also sound. \qed
\end{example}

For a set of abstract actions to be complete in this example, it must
contain actions to represent all concrete actions in the problem
such as stacking a block above $x$, picking up a block that is
not above $x$, and so on.
It may well be possible that a particular feature set $F$ cannot support
a set of abstract actions $A_F$ that is both sound and complete relative
to $\Q$. Soundness, however, is crucial for deriving general plans as it
ensures that in any state of any instance of $\Q$, an applicable abstract
action  can be ``instantiated'' by one or more concrete actions that have
the same qualitative effects on the features. 


\subsection{Solutions}

A solution to a generalized problem $\Q$ given the features $F$ and abstract
actions $A_F$ is a partial function $\pi$ that maps \emph{abstract states}
into \emph{abstract actions} such that $\pi$ solves all instances $P$ in $\Q$.
The plan or policy $\pi$ induces a trajectory $s_0,a_0,s_1, \ldots, s_n$ in
an instance $P$ of $\Q$ iff
1)~$s_0$ is the initial state in $P$,
2)~$a_i$ is one of the actions that instantiate $\pi(\bar{s}_i)$ in $s_i$,
3)~$a_i$ is applicable in $s_i$ and $s_{i+1}=f(a_i,s_i)$, and
4)~$s_n$ is a goal state of $P$, $n=\infty$, or $\pi(\bar{s}_n)$ either is
undefined, denotes an abstract action that is not applicable in $\bar{s_n}$,
or no applicable action $a_n$ instantiates $\pi(\bar{s}_n)$.
The policy $\pi$ solves $P$ iff all the induced trajectories reach the
goal of $P$.

\begin{example}
  A solution for $\Q_{clear}$ with feature set
  $F=\{H,n(x)\}$ is the policy $\pi$ given by the rules
  $\Rule{\neg H, n(x)>0}{\bar{a}}$ and $\Rule{H, n(x)>0}{\bar{a}'}$
  for the abstract actions $\bar{a}$ and $\bar{a'}$ above.
  The policy picks blocks above $x$ and puts them aside (not above $x$)
  until $n(x)$ becomes zero. \qed
\end{example}


\subsection{Computation}

The steps for obtaining policies $\pi$ for a generalized problem $\Q$
using the features $F$ and abstract actions $A_F$ are as follows \cite{bonet:ijcai2018}:

\begin{enumerate}[1.]
  \item The state variables $V_F$ and the abstract actions $A_F$ are
    extended with initial and goal formulas $I_F$ and $G_F$ over $V_F$
    to yield a \emph{numerical and non-deterministic planning problem}
    $Q_F=\tup{V_F,I_F,G_F,A_F}$. For soundness, $I_F$ must be such that
    the initial states of instances $P$ in $\Q$ all satisfy $I_F$, while
    all states that satisfy $G_F$ must be goal states of $P$.
    % \footnote{A   state $s$ is said to satisfy $I_F$ or $G_F$ if  $\bar{s}$ does.}
    % no longer needed
%
  \item The problem $Q_F=\tup{V_F,I_F,G_F,A_F}$ is converted into a
    \emph{boolean FOND} problem $Q'_F=\tup{V'_F,I'_F,G'_F,A'_F}$ by
    replacing the numerical variables $n\in N$ by the symbols $n=0$,
    the first-order literals $n=0$ by propositional literals  $n=0$, 
    the effects $n\pplus$ by deterministic effects $n > 0$, and the
    effects $n\mminus$ by non-deterministic effects $n>0\,|\,n=0$.
%
  \item The solutions computed by a FOND planner on $Q'_F$ are the
    strong cyclic solutions of $Q'_F$ \cite{strong-cyclic}.
    Such solutions however do not necessarily solve $Q_F$ because the
    non-deterministic effects $n>0\,|\,n=0$ in $Q'_F$ are not \emph{fair} but
    \emph{conditionally fair}: infinite decrements of $n$ ensure
    $n=0$ eventually \emph{only} when the number of increments of $n$ is finite.
    The problem of obtanining solutions of $Q'_F$ that only assume conditional
    fairness, the so-called terminating solutions in \cite{srivastava:aaai2011},
    is mapped into the problem of solving an amended FOND $Q^+_F$ that
     assumes standard  fairness \cite{bonet:ijcai2017}.\footnote{The
    translation to $Q^+_F$ is performed with software supplied by the authors.}
    %available by the authors that fixes bugs in the description found
    %in the paper at https://github.com/bonetblai/qnp2fond.}
\end{enumerate}

%The strong cyclic solutions of  $Q^+_F$ computed by FOND planners off-the-shelf   are solutions to the generalized problem $\Q_F$. 

\begin{example}
  Let restrict  $\Q_{clear}$  to those  instances where  the gripper is initially empty
  and there are blocks on top of $x$. For $F=\{H,n(x)\}$  and $A_F=\{\bar{a},\bar{a'}\}$ as above, 
  $Q_F=\tup{V_F,I_F,G_F,A_F}$ can be defined with $I_F=\{\neg H, n(x) > 0\}$ and $G_F=\{n(x)=0\}$.
  $Q'_F$ is like $Q_F$ but with $n(x)=0$ regarded as a propositional symbol, $n(x) > 0$
  as its negation, and the effect $n(x)\mminus$ of  $\bar{a}$ replaced by  $\neg n(x)=0 \, | \, n(x)=0$.
  Since  no action in $A_F$  increases $n(x)$,   $Q^+_F=Q'_F$, and the strong cyclic solution  of $Q'_F$,
  that corresponds to the policy above, solves $\Q_{clear}$. \qed
\end{example}


\section{Approximate Soundness and Completeness}

Bonet and Geffner's formulation and computational model rest on 
features $F$ and  sound abstract actions $A_F$ provided by hand.
The contribution of this work is to learn them  from samples:


\begin{definition}
  For a generalized problem $\Q$, a \emph{sample set}  $\S$ is a non-empty set of tuples  $(s,s',P)$
  such that 1)~$P$ is an instance of $\Q$, 2)~$s$ is a reachable state in $P$, 3)~$s'$
  is a possible succesor of $s$ in $P$; i.e., $s'=f(a,s)$ for some action $a$ applicable in $s$,
  4)~$\S$ is closed in the sense  that if $(s,s',P)$ is in $\S$, then $(s,s'',P)$ is in $\S$  if $s''$ is another
  succesor of $s$ in $P$.
\end{definition}

By assuming that sampled states $s$ are tagged with the instance $P$, 
we abbreviate the tuples $(s,s',P)$ as $(s,s')$, and refer to the
states $s'$ in  the pairs $(s,s') \in \S$ as the successors of
$s$ in $\S$. The  closure condition 4)~requires that
states $s$ appearing \emph{first} in pairs $(s,s')$ must be fully expanded in $\S$;
namely, all possible transitions $(s,s'')$ must be in the sample as well. We call them
the \emph{expanded} states in $\S$.

The features and abstract actions for $\Q$ are  learned by
enforcing an approximate form of soundness and completeness
over the samples. Like before,  we take a transition $(s,s')$ and an abstract action $\bar{a}$
to have  the same qualitative effects over a boolean  feature  iff they both make it true , they both make it false,  or
they both leave it unchanged,  and over a numerical feature  iff they both decrease it,  they both increase it,  or they both leave it unchanged.

\begin{definition}
  A set of abstract actions $A_F$ is sound \emph{relative to a  sample set} $\S$ for $\Q$
  iff for  any  abstract action $\bar{a}$ in $A_F$ that is applicable in  a expanded state $s$ of $\S$,
  there is a transition $(s,s')$ in $\S$ that has the same qualitative  effects over $F$ as $\bar{a}$.
\end{definition}

\begin{definition}
  A set of abstract actions $A_F$ is complete   \emph{relative to a sample set} $\S$ for $\Q$
  iff for each transition  $(s,s')$ in  $\S$, there is an abstract action $\bar{a}$ in $A_F$
  with the same qualitative effects over in $F$ that is applicable in $s$.
\end{definition}

For a sufficiently large sample set, these approximate notions converge to their exact versions.

\section{Learning  features and  abstract actions}

For computing the features and abstract actions we define a formula
$T(\S,\F)$ where ${\cal S}$ is a sample set for $\Q$ and $\cal F$
is a feature pool defined from the primitive domain predicates
as described below. The formula is  \emph{satisfiable} iff there is a set of
features $F \subseteq \F$ and a set of abstract actions $A_F$
over $F$ that is sound and complete relative to the sample set
$\S$.


\subsection{SAT Encoding}

For each transition  $(s, s') \in \S$ and each feature $f \in \F$,
$\Delta_f(s, s') \in \{+, -, \uparrow, \downarrow, \bot\}$ will denote the
\emph{qualitative change of value} of feature $f$ along the transition $(s, s')$,
which can go from false to true ($+$), from true to false ($-$) or remain
unchanged ($\bot$), for boolean features, and can increase ($\uparrow$),
decrease ($\downarrow$), or remain unchanged ($\bot$), for numerical features.
The \emph{propositional variables} in $T(\S,\F)$ are then:
\begin{enumerate}[{\small$\bullet$}]
  \item $selected(f)$ for each $f \in \F$, true iff $f$ selected (in $F$).
  \item $D_1(s,t)$ for states $s$ and $t$ expanded in $\S$,  true iff
    selected features distinguish $s$ from $t$; i.e.\ if $s$ and $t$ disagree
    on the truth value of a selected boolean feature $p$ or atom $n=0$ for a
    selected numerical feature $n$.
    %Since $D_1$ is symmetric, only symbols for $s < t$ are created for some
    %arbitrary, fixed order over the states in $\S_1$.
  \item $D_2(s, s', t, t')$ for each $(s, s')$ and $(t, t')$ in $\S$,
    true iff selected features $f$ distinguish the two transitions; i.e., if
    $\Delta_f(s, s')\not=\Delta_f(t,t')$. 
    %As for $D_1$, the number of $D_2$ variables is cut in half due to symmetries.
\end{enumerate}

\medskip
\noindent The first formulas in $T(\S,\F)$ capture the meaning of $D_1$:
  
\begin{equation}
  \label{eq:d1}
  D_1(s, t) \ \Leftrightarrow\ \textstyle \bigvee_{f}  selected(f)
\end{equation}

\noindent where $f$ ranges over the features in $\F$ with  different qualitative
values in $s$ and $t$; i.e.\ boolean features $p$ with different truth value
in $s$ and $t$, and numerical features $n$ with different truth values for
 $n=0$ in $s$ and $t$. 

\noindent The second formulas encode the meaning of $D_2$:

\begin{equation}
  \label{eq:d2}
  D_2(s, s', t, t') \ \Leftrightarrow\ \textstyle\bigvee_f  selected(f)
\end{equation}

\noindent where $f$ ranges over the features $f \in \F$ that have the same
qualitative values in $s$ and $t$ but which change differently in the two
transitions; i.e., $\Delta_f(s, s') \neq \Delta_f(t, t')$.

The third class of formulas relate $D_1$ and $D_2$ by enforcing \emph{soundness}
and \emph{completeness} over  the samples.  Since the abstract actions  are not given,
the qualitative changes in each  transition $(s,s')$ in $\S$
are  taken as templates  of abstract actions. Thus, if $s$ and $t$ are not distinguished by the selected features, 
for each transition $(s, s') \in \S$, there must be a transition $(t, t') \in \S$
such that that the two transitions are not distinguished by the selected features either.
This is expressed as

\begin{equation}
  \label{eq:bridge1}
  \neg D_1(s, t) \  \Rightarrow\ \textstyle\bigvee_{t'} \neg D_2(s, s', t, t')
\end{equation}

\noindent where $s$ and $t$ are expanded states in  $\S$, $s'$ is a successor of $s$ in $\S$,
and $t'$ ranges over the successors of $t$ in $\S$.

\Omit{ % left out: this is for efficiency only, and is not strictly needed. Also, doesn't fit as such with goal-marking
  Refer to this as an ``efficiency optimization'' ..
  
  such that $s < t$, and transition $(s,s')$ in $\S$. Due to the symmetry breaking caused by the constraint $s < t$,
  the following formulas are also needed

\begin{equation}
  \label{eq:bridge2}
  \neg D_1(s, t) \, \rightarrow \, \bigvee_{s'} \neg D_2(s, s', t, t')
\end{equation}

\noindent where the  iteration in the right hand disjunction goes over the different successor states $s'$ of the first argument
$s$ of $D_1$ that are in the sample set $\S$, $s$ and $t$ are states in $\S_1$ as before with $s < t$, and $t'$ is a
successor state of $t$ in $\S$; i.e. $(t,t') \in \S$.
}
%% --

The last set of clauses force the selected features to distinguish goal
from non-goal states as:
\begin{equation}
  \label{eq:goal}
  D_1(s,t) 
\end{equation}

\noindent where  $s$ and $t$  are expanded states  $\S$ such that exactly one of them is a goal state. 
For this, it is assumed that expanded states $s$ in $\S$ are labeled as goal or non-goal states.

The theory $T(\S,\F)$ given by the formulas  (\ref{eq:d1})--(\ref{eq:goal})
has a number of atoms that is linear in the size $|{\cal F}|$  of the feature pool $\F$,
and quadratic in the number $|{\cal S}_1$ of expanded  states $s$  and
in the number $|{\cal S}|_2$ of transitions $(s,s')$ in $\S$.
The resulting number of clauses grows   with
$|{\cal S}_1|^2 \times ({\cal F} + {\cal S}_2)$, and 
can be roughly cut in half by  symmetry considerations.

In the experiments we also consider a smaller theory that avoids the
consideration of all pairs of expanded states or all pairs of transitions
in $\S$. For this, some expanded states $s$ and transitions $(s,s')$ in
tuples $(s,s',P)$ are marked as \emph{goal relevant}. This is achieved by
computing some plans for some instances $P$ in $\Q$, and by adding the
resulting state transitions to $\S$ at the same time that the first
arguments $s$ and $(s,s')$ in the $D_1$ and $D_2$ atoms are restricted
to be marked.
If the set of marked states and transitions is much smaller than the set of unmarked ones, 
then the resulting formula,  $T_G(\S,\F)$ will be much smaller than $T(\S,\F)$.
The goal marking scheme that involves the theory $T_G(\S,\F)$ in place of $T(\S,\F)$
amounts to enforcing soundness on  the sample set $\S$ but not completeness, yet this does
not affect the resulting  formal guarantees that depend on soundness only. 


\subsection{Extracting $F$ and  $A_F$}

For a satisfying assignment $\sigma$ of the theory $T(\S,\F)$, let
$F_{\sigma}$ be the set of features $f$ in $\F$ such that $selected(f)$
is true in $\sigma$, and let $A_{\sigma}$ be the set of abstract actions
that capture the transitions $(s,s')$ in $\S$, with duplicates and redundancies removed.
The abstract action $\bar{a}=\abst{Pre}{\Eff}$ that captures the transition $(s,s')$
has the \emph{precondition} $p$ (resp.\ $\neg p$) if $p$ is a boolean feature in
$F_{\sigma}$ that is true (resp.\ false) in $s$, and has the precondition $n=0$
(resp.\ $n > 0$) if $n$ is a numerical feature in $F_{\sigma}$ such that $n=0$
is true (resp.\ false) in $s$. Similarly, $\bar{a}$ has the \emph{effect} $p$
(resp.\ $\neg p$) if $p$ is a boolean feature in $F_{\sigma}$ that is true
(resp.\ false) in $s'$ but false (resp.\ true) in $s$, and the effect $n\mminus$
(resp.\ $n\pplus$) if $n$ is a numerical feature in $F_{\sigma}$ whose values
decreases (resp.\ increases) in the transition from $s$ to $s'$. 
If two abstract actions $\bar{a}$ and $\bar{a}$ differ only in the sign of a
precondition, the precondition can be dropped, removing the duplicates and
iterating until no more simplifications can be done. They key properties are:

\begin{theorem}
  The theory $T(\S,\F)$ is satisfiable iff there is a set of features $F \subseteq \F$
  and a set of abstract actions $A_F$ over $F$ such that $A_F$ is sound and complete relative to  $\S$.
\end{theorem}

\begin{theorem}
  If $\sigma$ is a satisfying assigment of $T(\S,\F)$, 
  $A_{\sigma}$ is sound and complete relative to $\S$.
\end{theorem}

\begin{theorem}
  If $\sigma$ satisfies  $T_G(\S,\F)$, $A_{\sigma}$  is sound relative to $\S$.
\end{theorem}

\Omit{
%% commented out
The goal expression $G_F$ can be provided explicitly  in terms of the primitive predicates,
but  more generally can be extracted  from the satisfying assigment $\sigma$ as well.
For this, $G_F = G_{\sigma}$ is defined as the DNF formula whose terms 
correspond to the abstract states over the selected features in $F_{\sigma}$ 
that are false in all expanded states in $\S$ marked as non-goals
and true in some expanded goal state. This DNF formula can   be simplified in standard ways. 
Due to  (\ref{eq:d1}) and (\ref{eq:goal}) that force the selected features in $F_{\sigma}$
to   distinguish goal  from non-goal states, we have that:

\begin{theorem}
For a satisfying assigment $\sigma$  of $T(\S,\F)$, 
$s$ is expanded goal   state in $\S$  iff  $s$ satisfies  $G_F=G_{\sigma}$.
\end{theorem}
}
% 
Different satisfying assignments $\sigma$ give rise to different features $F_{\sigma}$
and abstract actions $A_{\sigma}$. More meaningful features are found by looking for
satisfying assignments $\sigma$ that minimizes a cost measure such as $|F_{\sigma}|$
or more generally $\sum_{f \in F_{\sigma}} cost(f)$. We achieve this
optimization through the use of Min SAT or Max-Weighted SAT solvers, where it suffices
to give infinite weights to the clauses resulting from $T(\S,\F)$ and weights
given by $cost(f)$ to the ``soft'' unit clauses $\neg selected(f)$.


\section{Feature Pool}

The computation of the feature pool $\F$ used in $T(\S,\F)$
relies on the predicates used to encode the instances in $\Q$.
A simple grammar allows  us   to  construct new predicates 
and to define  the boolean and numerical features in $\F$
in terms of them. The key issue is how to the define
new predicates. For this, we take a solution off-the-shelf: \emph{description logics (DL)},  also called
\emph{concept languages},  are grammars for doing  this.
Concepts $C$  refer  to predicates of arity 1 and roles $R$  to predicates of arity 2.
Concept languages have been used before for  generating general policies using purely learning methods  \cite{martin:concept,fern:bias}.
\citeay{dl-handbook} give syntax and semantics of description logics.


\subsection{Grammar}

We use a standard  DL grammar  given by the rules:
\begin{enumerate}[{\small$\bullet$}]
  \item $C \ \leftarrow\ \ C_p, C_u, C_x$: primitive, univ.,  nominal concepts,\footnote{Denotation of $C_x$ for parameter $x$ is $\{x\}$.}
  \item $C \ \leftarrow\  \neg C, C \sqcap C'$: negation and conjunction, 
  \item $C \ \leftarrow\  \exists R.C , \forall R.C$: concepts that use of  roles: first denotes $x$ s.t. for some  $y$,  $R(x,y) \land C(y)$.
  Second,  $x$ s.t. for all $y$, $R(x,y) \rightarrow C(y)$,
  \item $C \ \leftarrow\  R=R'$: stands for $x$ s.t. set of $y$'s  s.t. $R(x,y)$ equal to set of $y$'s s.t. $R'(x,y)$,
    \item $R \leftarrow R_p, R^{-1}, R^*$: primitive, inverse, transitive closure.
%     \item $R \leftarrow R \circ R'$, role composition: $(x,y)$ in $R \circ R'$ if $(x,z)$ in $R$ and $(z,y)$ in $R'$.
%     \item $R \leftarrow R:C$, denotes pairs $(x,y)$ in $R$ s.t. $C(y)$.
 \end{enumerate}

The  compositional  semantics   of these rules allows us to determine the denotations (extensions)   $C^s$ and $R^s$  of
the resulting unary and binary predicates $C^s$ and $R^s$ from the denotation of the primitive predicates
in any  state $s$. We will see examples of derived concepts below such as ${\exists on^* . C_x}$,
whose denotation in a state $s$ of the Blockworld is the set of blocks that are above $x$ in $s$.
The numerical features $n_C$ will be associated with the cardinality of the extension $C^s$. 


\subsection{Pool}

The set of all concepts and roles with complexity no larger than $k$ is
denoted as $\G^k$. The complexity of a concept or role is the minimum
number of rules needed to generate it. The size of $\G^k$ is reduced by
incrementally pruning ``duplicate'' concepts; namely, those whose denotation
coincides with the denotation of another concept over all the sampled states.
%in $\S$. 

The pool $\F=\F^k$  is defined from $\G^k$ and contains
the  features $f$ that represent the functions $\phi_f(s)$ as follows:
\begin{enumerate}[{\small$\bullet$}]
\item Boolean $b_p$ for each primitive predicate $p$ of arity $0$,
\item Boolean $b_C$ for each $C$ that are true in $s$ iff $|C^s| > 0$,
\item Numerical $n_C$ for each $C$ that stand for cardinality $|C^s|$,
\item Numerical $\textit{dist}(C_1,R:C,C_2)$, that represent that smallest
  $n$ such that for $x_1, \ldots, x_n$, $C_1^s(x_1)$, $C_2^s(x_{n})$, and
  $(R:C)(x_i,x_{i+1})$ for $i=1, \ldots, n$. 
\end{enumerate}  

The role $R:C$ is such that its denotation contains the pairs $(x,y)$ in $R$
such that $y$ is in $C$.
Some pruning is done in this space of features as well. E.g., numerical
features $n_C$ are pruned from the pool when there are no states $s$ and $s'$
in the sample set for which $n_c(s)=0$ and $n_c(s') > 1$, while boolean
features $n_b$ are pruned if uniformly true or false for all $s$, or if $n_C$
is not pruned. 
The measure $cost(f)$ is set to the complexity of $C$ for $n_C$ and $b_C$,
to $0$ for $b_p$ and to the complexities of $C_1$, $R$, $C$, and $C_2$ added
up for $\textit{dist}(C_1,R:C,C_2)$.
Only features with cost bounded by $k$ are allowed in $\F$.


\section{Computational Model: Summary}

In summary, the steps for computing general plans are:
\begin{enumerate}[1.]
  \item a sample set $\S$ is computed from a few instances $P$ of a given
    generalized problem $\Q$,
  \item a pool of features $\F$ is obtained from the predicates used in the
    instances $P$, the grammar, a  bound $k$, and the sample $\S$ (used for
    pruning),
  \item a satisfying assignment $\sigma$ of the theory $T(\S,\F)$ or $T_G(\S,\F)$
    that minimizes $\sum_{f \in F_{\sigma}} cost(f)$ is obtained using a
    Weighted-MAX SAT solver,
  \item the features $F$ and abstract actions $A_F$ are extracted from $\sigma$
    as $F_{\sigma}$ and $A_{\sigma}$,
  \item the problem $Q_F=\tup{V_F,I_F,G_F,A_F}$ is defined with initial and goal
    conditions $I_F$ and $G_F$ provided by hand to match $\Q$, 
  \item the FONDP $Q^+_F$, constructed automatically from $Q_F$, is solved by an off-the-shelf
    FOND planner.
\end{enumerate}

Notice that the policy $\pi$ that results from the last step deals with
propositional symbols that correspond to atoms $p$ and $n=0$ for boolean
and numerical features $p$ and $n$ in $F$.
When this policy is applied to an instance $P$ of $\Q$, however, such features denote state functions
which are determined by the  semantics of the concepts and roles involved
(e.g., $n_C$ stands for feature function $\phi_{n_c}(s)=|C^s|$).
The exception are the boolean features $b_p$ associated with
predicates $p$ of zero arity which are treated as fluents.

If we write $A \vDash B$ to express that all the states that satisfy $A$, satisfy $B$, 
the formal guarantees can be expressed as:

\begin{theorem}
If the abstract actions $A_F$ that are sound relative to the sample set $\S$  are actually sound,
then a   policy $\pi$ that solves  $Q^+_F$  is guaranteed to solve  all instances $P$ of $\Q$
with initial and goal situations $I$ and $G$ such that $I \vDash  I_F$ and $G_F \vDash G$.
\end{theorem}

This means that a policy $\pi$ obtained with a FOND planner from $Q^+_F$ 
solves the generalized problem $\Q$, provided that the abstract actions
learned are sound and that $\Q$ is restricted to the instances $P$ whose
initial and goal conditions comply with the conditions on $I_F$ and $G_F$.
% \textcolor{red}{\bf ** NEXT IS UNCLEAR **}
% ok, I removed this. It's not strictly needed and we don't use it
\Omit{
Since we do not use a language for specifying $\Q$ formally, we will 
take the formulas $I_F$ and $G_F$  as a partial formal specification of
$\Q$. The theorem then says that if soundness over the samples, ensure
formal soundness over $\Q$, the policy that solves the FONDP $Q^+_F$,
solves $\Q$. The result holds whether $T$ or $T_G$ (goal marking) is used in step 3.
In principle, we could learn the formulas $I_F$ and $G_F$ from the sample as well,  but with different guarantees. 
}


\section{Experimental Results}

We evaluate the computational model on 4 different generalizd planning problems.
%We evaluate the computational model described in steps 1--6 above over 4 different domains.
%
% *Fill up general details in 1--6 that don't change from problem to problem
%how $\S$ obtained
%The sample set $\S$
% s obtained following one of two possible strategies.
% In the \emph{exhaustive} sampling strategy, $\S$
For each generalized problem $\Q$, a number of training instances $P$ in $\Q$
are selected and explored in order to generate the sample set $\S$. %that is used to learn the features and actions for $\Q$.
For each such $P$, $\S$ contains the first $m$ states that are expanded by
breadth-first search from the initial state in $P$, plus all the states along
an optimal plan $\pi^*$ from the initial state of each such $P$ \textcolor{red}{**** for each $P$? ****}
In addition, $\S$ is closed by adding the transitions $(s,s')$ for all states
$s$ in $\S$ such that $s'$ not in $\S$.
For the goal marking scheme that yields the smaller theory $T_G(\S, \F)$,
the states that are marked as goal-relevant are the states reached by
the optimal plan $\pi^*$.

%
% In the \emph{random} sampling strategy, a large number $M$ of states are generated, potentially exhausting the full state space,
% and then a certain number $m \ll M$ of them is sampled uniformly at random. If no goal state is found among those $m$,
% one random goal state from the larger set is drawn at random and added into the sample.
% \textcolor{red}{\bf **** What if $M$ doesn't have a goal? ****}

The process to generate the pool of features is the same across all the
experiments except that distance features are generated only for the
last example. A complexity bound $k=8$ is used to limit the size of the
pool.
The resulting theories are then solved with the Open-WBO MAXSAT solver
\cite{martins2014open}.

The learnt abstractions are passed through the provided QNP translator 
to generate FOND problems which are solved using a SAT-based FOND
planner \cite{tomas:fond-sat}. The translation is in all cases very fast
(i.e., $\sim 0.01$ seconds) as it runs in low polynomial time in the size
of the abstraction. Solving the FOND problem, on the other hand, is
more demanding.

%the value $k=8$ in all the expetimenin the feature pool $\mathcal{F} = \mathcal{F}^k$ is set to a value of $k=8$,
%with just one difference in the concept generation process: distance features are only generated for the last example,
%meant to illustrate their usefulness.
% In our experiments, we increased this value to $k=16$ if no model was found
% for the logical theories $T$ and $T_G$, and although it would be more effective
% to devise an adaptive strategy to do so incrementally or through binary search,
% that is beyond the scope of this work.
% theory $T({\cal S},{\cal F})$, in which case it is increased to $k=20$.
% We have considered strategies to increase the bound adaptively

%\alert{Details on the FOND planner, QNP translation, etc.}

Very few training instances $P$ are used to generate the sample sets,
and each such instance is also small. The instances were manually
chosen in order to display interesting situations, yet we expect
similar results can be obtained in a fully automatic manner by 
increasing and diversifying the set of training instances.

The whole computational pipeline is processed on Intel Xeon E5-2660 CPUs
with time and memory cutoffs of 1h and 32GB.
Table~\ref{tab:exp-result-data} summarizes the relevant data for each
of the benchmarks.


%by choosing the training instances $P$ manually, like a teacher. 
%Very few instances turn out to be required for generating general plans.
%The learning scheme is not supervised, as the training set are state
%transitions, not state action pairs to be generalized; but is not fully
%unsupervised either as currently implemented, with the instances $P$ from
%$\Q$ selected by hand.
%We do not expect the results to be different if such instances were to be
%sampled at random, but the number of instances required for producing sound
%abstract actions and valid general plans, as well as the size of the
%theories, would likely be higher.



\newcommand{\rowspacing[1]}{&&&&&&&&&&&&&&&&\\[\dimexpr-\normalbaselineskip+2pt]}

\begin{table*}[t]
  \centering
  %\resizebox{.95\textwidth}{!}{
    \begin{tabular}{lrrrrr@{}rrrrrrrr}
      \toprule
                     &     &            &            & \multicolumn{2}{c}{$T(\S,\F)$} && \multicolumn{2}{c}{$T_G(\S,\F)$} \\
      \cmidrule{5-6}
      \cmidrule{8-9}
                     & $n$ & $\abs{\S}$ & $\abs{\F}$ &   $np$ &   $nc$ &&   $np$ &   $nc$ & $t_{\text{SAT}}$ & $\abs{F}$ & $\abs{A_F}$ & $t_{\text{FOND}}$ & $\abs{\pi}$ \\
      \midrule
      $\Q_{clear}$   &   1 &        927 &        322 &   535K &  59.6M &&   7.7K &   767K &             0.01 &         3 &           2 &              0.46 &           5 \\
      $\Q_{on}$      &   3 &        420 &        657 &   128K &  25.8M &&  18.3K &   3.3M &             0.02 &         5 &           7 &              7.56 &          12 \\
      $\Q_{gripper}$ &   2 &        403 &        130 &    93K &   4.7M &&   8.1K &   358K &             0.01 &         4 &           5 &            171.92 &          14 \\
      $\Q_{reward}$  &   2 &        568 &        280 &   184K &  11.9M &&  15.9K &   1.2M &             0.01 &         2 &           2 &              1.36 &           7 \\
      \bottomrule
    \end{tabular}
  %}
  \caption{Results on 4 different benchmarks: $n$ is number of training instances,
    $\abs{\S}$ is number of transitions in $\S$,
    %$k$ is max.\ feature comlexity,
    $\abs{\F}$ is size of feature pool,
    $np$ and $nc$ are the number of propositions and clauses in theories $T(\S,\F)$ and $T_G(\S,\F)$,
    $t_{\text{SAT}}$ is time for SAT solver on theory, 
    $\abs{F}$ is number of selected features,
    $\abs{A_F}$ is number of abstract actions,
    $t_{\text{FOND}}$ is time for the FOND planner, and
    $\abs{\pi}$ is the size of the found solution.
    %$time_{\text{SAT}}$, $T_{\text{FOND}}$: computation time (sec.) of the SAT and FOND solvers, resp.;
    %$T_{\text{trans}}$: time (sec.) to generate the FOND problem $Q^+_F$;
    %$\#Act$, $\#At$: number if actions and atoms in $Q^+_F$;
    All times are in seconds. %figures that depend on the SAT theory are given for $T_G(\S,\F)$.
  }
  \label{tab:exp-result-data}
\end{table*}


% For each problem: 1--6 details that vary from problem to problem. Follow same structure:
% $\Q$, $P$s, primitive predicates, number of atoms and clauses  of $T$ and/or $T^G$,
% SAT times,  $F$, $|A_F|$;  number of FOND atoms and actions in $Q^+_F$ (include those in $F$, $A_F$ plus translation), $I_F$ and $G_F$,
% FOND time, resulting policy.  Numbers can go all in a table.


\paragraph{Clearing a block.}
The problem $\Q_{clear}$ aims at reaching the goal $clear(x)$ for a
block $x$ in Blocksworld. We use the standard encoding that involves
the primitive predicates $on(\cdot,\cdot)$, $clear(\cdot)$,
$ontable(\cdot)$, $holding(\cdot)$, and $handempty$, and stack/unstack
actions.
Using one single training instance of 5 blocks, we are able to learn
a sound (but incomplete) abstract model with the two abstract
actions:\footnote{The action names are not learnt but provided by us
  to make their meaning explicit.}
\begin{enumerate}[--]
  \item $\text{put-aside} = \abst{\neg X, H}{\neg H}$,
  \item $\text{pick-above-$x$} = \abst{\neg X, \neg H, n(x) > 0}{H, n(x)\mminus}$.
\end{enumerate}
The abstract model contains two boolean features $X$ and $H$, and
a single numerical feature $n(x)$. 
The first feature tracks whether the gripper holds block $x$, the
second whether the gripper holds any block (including $x$), and the
last tracks the number of blocks above $x$.
These features are expressed in DL as:
\begin{enumerate}[--]
  \item $H: \abs{holding}$, \textcolor{red}{Looks numerical because card..}
  \item $X: \abs{holding \sqcap \set{x}}$,
  \item $n(x): \abs{\exists\,on^* . \set{x}}$. \textcolor{red}{Explain?}
\end{enumerate}

The translator produces a FOND problem in 0.01 seconds that is solved
by the planner in 0.46 seconds.
The resulting plan makes a loop that applies pick-above-$x$ followed
by put-aside until $x$ becomes clear, yet in the last iteration the
held block is not put aside. The initial and goal situations are
$\neg X,\neg H,n(x)>0$ and $\neg X,n(x)=0$ respectively.


% Action 1: PLACE-ASIDE
%         PRE: NOT holding(a), bool[holding]
%         EFFS: NOT bool[holding]
% 
% Action 2: PICK-FROM-A
%         PRE: NOT bool[holding], NOT holding(a), n(a) > 0
%         EFFS: DEC n(a), bool[holding]

% Loop:
%   Action 2 [Pick block above x]
%   If n(a) = 0 Break
%   Action 1 [Put block aside]
%

\paragraph{Stacking two blocks from different towers.}
The problem $\Q_{on}$ consists in achieving the goal $on(x,y)$
in Blocksworld for any two blocks $x$ and $y$ that are not
\emph{initially} not in the same tower.
Using the above Blocksworld encoding and only 3 different training
instances with up to 14 blocks, an abstract model involving 3 boolean
features, 2 numerical features, and 7 actions is learnt.
The boolean features are $E$, $X$ and $on(X,Y)$ that stand for the gripper
being empty, the gripper holding block $X$, and the block $x$ being directly on
block $y$, while the learnt numerical features are $n(x)$ and $n(y)$ that stand
for the number of blocks above $x$ and $y$ respectively.
The features $X$, $n(x)$, and $n(y)$ are specified in DL as above,
while $E: handempty$ and $on(X,Y): \abs{(\exists\,on^{-1}.\{x\}) \sqcap \{y\}}$.
Interestingly, this last feature is the concept-based representation
of the goal $on(x,y)$.

%% blocks
%% 5 bool[handempty] 0 n(a) 1 n(b) 1 holding(a) 0 on(a,b)_2 0
%% 5 bool[handempty] 1 holding(a) 0 n(a) 1 n(b) 1 on(a,b)_2 0
%% 4 holding(a) 0 n(a) 0 n(b) 1 on(a,b)_2 1
%% 7
%% action_1 ; put held block (diff. from a and b) aside
%% 4 bool[handempty] 0 holding(a) 0 n(a) 0 on(a,b)_2 0
%% 1 bool[handempty] 1
%% action_2 ; put aside
%% 5 bool[handempty] 0 holding(a) 0 n(a) 1 n(b) 1 on(a,b)_2 0
%% 1 bool[handempty] 1
%% action_3 ; pick block a
%% 5 bool[handempty] 1 holding(a) 0 n(a) 0 n(b) 0 on(a,b)_2 0
%% 2 bool[handempty] 0 holding(a) 1
%% action_4 ; put a (being held) aside
%% 5 bool[handempty] 0 holding(a) 1 n(a) 0 n(b) 1 on(a,b)_2 0
%% 2 bool[handempty] 1 holding(a) 0
%% action_5 ; pick block different from b that is above a
%% 5 bool[handempty] 1 holding(a) 0 n(a) 1 n(b) 1 on(a,b)_2 0
%% 2 bool[handempty] 0 n(a) 0
%% action_6 ; pick block different from a that is above b
%% 5 bool[handempty] 1 holding(a) 0 n(a) 0 n(b) 1 on(a,b)_2 0
%% 2 bool[handempty] 0 n(b) 0
%% action_7 ; put a directly on b
%% 5 bool[handempty] 0 holding(a) 1 n(a) 0 n(b) 0 on(a,b)_2 0
%% 4 bool[handempty] 1 holding(a) 0 n(b) 1 on(a,b)_2 1

The set of 7 abstract actions includes actions to pick up the clear
block above $x$ or $y$, to put aside the block being held (not above
$x$ nor $y$), to pick block $x$, to place $x$ on $y$, and to place
$x$ on the table.

The translator takes 0.01 seconds and the planner computes a
solution in 7.56 seconds. The policy loops to clear block $x$, then
loops to clear block $y$, and finishes by picking $x$ and
placing it on block $y$.

\Omit{ % CONTAINS FEATURES AND ACTIONS
\begin{itemize}
\item $X$, $n(x)$, $n(y)$, as in $\Q_{clear}$,
\item $E: handempty$, whether gripper is empty,
\item $XY: [(\exists on^{-1} . \set{x})\sqcap \set{b}]$, whether $x$ is on $y$.
\end{itemize}

\noindent Interestingly, the last feature is a concept-based representation of the actual STRIPS goal $on(x,y)$.
The corresponding action model $A_F$ is:

\begin{itemize}
% ATM I use two lines, as doesn't fit in one :-(
\item \emph{pick-above-$x$}: \\\hspace*{\fill}  \tuple{\neg X, \neg XY, E, n(x) > 0, n(y)>0; n(x) \downarrow, \neg E  }

\item \emph{pick-above-$y$}: \\\hspace*{\fill}  \tuple{\neg X, \neg XY, E, n(x) = 0, n(y)>0; n(y) \downarrow, \neg E  }

\item \emph{put-$x$-on-$y$}: \\\hspace*{\fill} \tuple{X, \neg XY, \neg E, n(x)=n(y)=0 ; n(y) \uparrow, \neg X, E, XY}

\item \emph{pick-$x$}:         \hspace*{\fill}  \tuple{\neg X, \neg XY, E, n(x)=0, n(y)=0; E, X}

\item \emph{put-$x$-table}:                     \tuple{\neg E, \neg XY, X, n(x)=0, n(y) > 0; \neg X, E}

\item \emph{put-aside-1}:      \hspace*{\fill} \tuple{\neg E, \neg X, \neg XY, n(x) = 0; E}

\item \emph{put-aside-2}:      \hspace*{\fill} \tuple{\neg E, \neg X, \neg XY, n(x) > 0, n(y) > 0; E}
\end{itemize}

% Action 1 [PUT-ASIDE-1]:
%         PRE: NOT bool[handempty], NOT holding(a), NOT on(a, b)_2, n(a) = 0
%         EFFS: bool[handempty]
% 
% Action 2 [PUT-ASIDE-2]:
%         PRE: NOT bool[handempty], NOT holding(a), NOT on(a, b)_2, n(a) > 0, n(b) > 0
%         EFFS: bool[handempty]
% 
% Action 3 [PICK-A]:
%         PRE: NOT holding(a), NOT on(a, b)_2, bool[handempty], n(a) = 0, n(b) = 0
%         EFFS: NOT bool[handempty], holding(a)
% 
% Action 4 [PUT-A-ONTABLE]:
%         PRE: NOT bool[handempty], NOT on(a, b)_2, holding(a), n(a) = 0, n(b) > 0
%         EFFS: NOT holding(a), bool[handempty]
% 
% Action 5 [PICK-FROM-ABOVE-A]:
%         PRE: NOT holding(a), NOT on(a, b)_2, bool[handempty], n(a) > 0, n(b) > 0
%         EFFS: DEC n(a), NOT bool[handempty]
% 
% Action 6 [PICK-FROM-ABOVE-B]:
%         PRE: NOT holding(a), NOT on(a, b)_2, bool[handempty], n(a) = 0, n(b) > 0
%         EFFS: DEC n(b), NOT bool[handempty]
% 
% Action 7 [PUT-A-ON-B]:
%         PRE: NOT bool[handempty], NOT on(a, b)_2, holding(a), n(a) = 0, n(b) = 0
%         EFFS: INC n(b), NOT holding(a), bool[handempty], on(a, b)_2

Action \emph{put-$x$-table} will indeed be necessary only in case block $x$ starts being held
but block $y$ is not clear.
%
The two last \emph{put-aside} actions show that when using the goal-oriented theory $T_G(\S,\F)$
it is not always possible to completely merge similar actions.
This is because some states (in this case, any state with $n(x) > 0 \land n(y) = 0$ and some block $b \not\in \set{x,y}$ being held)
by chance did not appear on the optimal plans computed, which makes the resulting $A_F$ not complete in general.
This limitation could be circumvented either by manually providing an additional 
instance fulfilling that condition, or, more robustly, by using a larger set of optimal plans instead of just one.

Interestingly, action \emph{pick-$x$} illustrates a different type of incompleteness, as it will
only pick block $x$ whenever block $y$ is clear. No alternative optimal plan will possibly fix this,
since (provided $x$ and $y$ are initially not in the same tower) no optimal plan for $on(x,y)$ will pick block $x$ before $y$ is clear.
This incompleteness however will make the computation of the policies easier and the resulting policies likely more effective.


The policy ...
\alert{Display policy here}
}


\paragraph{Gripper.}
The problem is based on the classical Gripper problem where a robot with
two grippers needs to move a number of balls from one room into another
\emph{target} room $x$. Each gripper of the robot may carry one ball 
at a time, and the number of balls to move depends on the instance of
the problem.
We use the standard encoding with the primitive predicates $at\text{-}robby(l)$,
$at\text{-}ball(b, l)$, $free(g)$, $carry(b, g)$ that denote, respectively,
whether the robot (the ball) is at location $l$, whether gripper $g$ is free,
and whether gripper $g$ carries ball $b$, plus unary type predicates $room$,
$ball$, and $gripper$.

The pipeline is fed with 2 training instances with 2 rooms and at most
4 balls. The resulting abstract model contains the features:
\begin{enumerate}[--]
  \item $X: at\_robby \sqcap \set{x}$ (whether robby is in target room),
  \item $B: \exists\,at . \neg \set{x}$ (number of balls not in target room),
  \item $C: \exists\,carry . \top$ (number of balls carried by robby),
  \item $G: \abs{free}$ (number of empty grippers).
\end{enumerate}
The abstract actions in the learnt model are:
\begin{enumerate}[--]
  \item $\text{drop-ball-at-$x$}=\abst{C>0, X}{C\mminus, G \pplus}$,
  \item $\text{move-to-$x$-half-loaded}\! =\!  \abst{\neg X, B=0, C>0, G\!>\!0}{X}$,
  \item $\text{move-to-$x$-fully-loaded} = \abst{\neg X, C>0, G=0}{X}$,
  \item $\text{pick-ball-not-in-$x$} = \abst{\neg X, B > 0, G > 0}{B\mminus, G\mminus, C\pplus}$,
  \item $\text{leave-$x$} = \abst{X, C=0, G > 0}{\neg X}$.
\end{enumerate}

As before, the model is sound and incomplete, but it remains sound
for versions of the problem that involve more than two grippers,
and the found solution also solves such problems.

The problem is translated into FOND in 0.01 seconds and the
planner finds a solution in 171.92 seconds.

\textcolor{red}{\bf *** NEED POLICY ***}

\Omit{%BLAI
The problem $\Q_{gripper}$ is based on the classical Gripper problem, where a robot with two grippers needs to 
move a number of blocks from one room into another \emph{target} room $x$, which we take to be a parameter of the problem,
each gripper of the robot being able to carry only one ball at a time.
We use the standard STRIPS representation with primitive predicates $at\text{-}robby(l)$, $at\text{-}ball(b, l)$, $free(g)$, $carry(b, g)$ that denote,
respectively, whether the robot (the ball) is at location $l$, whether gripper $g$ is free, and whether gripper $g$ carries ball $b$,
plus unary type predicates $room$, $ball$, and $gripper$.
%
Using 2 training instances $P$ with only 2 rooms and at most 4 balls, we learn an abstract state space
based on the following features:

\begin{itemize}
\item $C: \abs{\exists carry . \top}$, number of balls carried by the robot, 
\item $X: at\_robby \sqcap \set{x}$, whether robot in target room,
\item $G: \abs{free}$, number of empty grippers, and
\item $B: \abs{\exists at . \neg \set{x}}$, number of balls not in target room,
\end{itemize}


\noindent and an abstract action model $A_F$:


\begin{itemize}
\item \emph{drop-ball-at-$x$}: \hspace*{\fill} \tuple{C>0, X;  C\downarrow, G \uparrow}
\item \emph{move-to-$x$-fully-loaded}: \hspace*{\fill} \tuple{\neg X, C>0, G=0;   X}
\item \emph{move-to-$x$-half-loaded}: \hspace*{\fill} \tuple{\neg X, B=0, C>0, G>0;   X}
\item \emph{pick-ball-not-in-$x$}: \hspace*{\fill} \tuple{\neg X, B > 0, G > 0;   B \downarrow, G \downarrow, C\uparrow}
\item \emph{leave-$x$}: \hspace*{\fill} \tuple{X, C=0, G > 0;   \neg X}
\end{itemize}

The marking of only states in optimal plans as \emph{goal-relevant} introduces again a bias
that can be seen e.g. in the fact that the robot is not allowed to leave room $x$ if carrying any ball,
or that \emph{move-to-$x$-half-loaded} is only allowed in the last move, when no more balls remain in the other room.
Although allowing moves without full load before that would help minimizing the action model and achieving completeness, it would certainly \emph{not}
help obtaining better policies.
%
Despite this incompleteness, the action model is \emph{provably correct for any $\Q_{gripper}$ instance with an arbitrary number of balls, 
rooms and grippers}.


\alert{Display policy here}
}


% Action 1:
%         PRE: ncarried > 0, robot-at-B
%         EFFS: DEC ncarried, INC nfree-grippers
% 
% Action 2: [move-to-x-half-loaded]
%         PRE: NOT robot-at-B, nballs-A = 0, ncarried > 0, nfree-grippers > 0
%         EFFS: robot-at-B
% 
% Action 3: [move-to-x-fully-loaded]
%         PRE: NOT robot-at-B, ncarried > 0, nfree-grippers = 0
%         EFFS: robot-at-B
% 
% Action 4:
%         PRE: NOT robot-at-B, nballs-A > 0, nfree-grippers > 0
%         EFFS: DEC nballs-A, DEC nfree-grippers, INC ncarried
% 
% Action 5:
%         PRE: ncarried = 0, nfree-grippers > 0, robot-at-B
%         EFFS: NOT robot-at-B




\paragraph{Collecting rewards in a maze.}
The problem consists in an agent that needs to navigate a rectangular
grid to pick up rewards that are spread throughout the grid, while 
avoiding cells that are considered as \emph{blocked}.
The encoding has primitive predicates $reward(c)$, $at(c)$, $blocked(c)$
and $adjacent(c_1, c_2)$ that denote the position of the rewards, of
the agent, of the blocked cells, and the grid topology respectively.
The computational pipeline is fed with two instances of sizes $4 \times 4$
and $5\times 5$ and given distributions of blocks and rewards.
A set featuring two numerical features is learnt:
\begin{enumerate}[--]
  \item $R: \abs{reward}$ (number of remaining rewards), and
  \item $D: dist(at, adjacent:(\neg blocked), reward)$ (distance from current
    cell to closest cell with reward, traversing adjacent, unblocked cells only).
\end{enumerate}
The learnt action model is:
\begin{enumerate}[--]
  \item $\text{pick-reward} = \abst{D=0, R > 0}{R\mminus, D\pplus}$,
  \item $\text{move-towards-closest-reward} = \abst{R>0, D>0}{D\mminus}$.
\end{enumerate}

The problem is translated into FOND in 0.01 seconds and the
planner finds a solution in 1.36 seconds.

\textcolor{red}{\bf *** NEED POLICY ***}

\Omit{
The objective in the problem $\Q_{reward}$ is to move in an $n \times m$ grid to pick up certain
rewards that are spread out in certain grid cells, while avoiding other cells which are considered as \emph{blocked}.
This example is from recent work in symbolic deep
reinforcement learning, where there is a penalty associated for
stepping in blocked cells instead \cite{garnelo2016towards}.
%
We represent the instances $P$ with primitive predicates $reward(c)$, $at(c)$, $blocked(c)$ and $adjacent(c_1, c_2)$ that denote
respectively the position of the rewards, of the agent, of the blocked cells, and the grid topology.
This is the only problem where we test the $\textit{dist}(C_1,R:C,C_2)$ min-distance features introduced before.
We use two single training instances with grid sizes $5 \times 5$ and $4 \times 4$ and different distributions of blocks and rewards,
and learn an abstraction with 2 features:

\begin{itemize}
\item $R: \abs{reward}$, number of remaining rewards, and
\item $D: dist(at, adjacent:(\neg blocked), reward)$, distance from current cell to closest cell with reward, traversing adjacent, unblocked cells only.
\end{itemize}

\noindent The action model has the following 2 actions:

\begin{itemize}
\item \emph{pick-reward}: \hspace*{\fill} \tuple{D=0, R > 0;  R \downarrow, D \uparrow}
\item \emph{move-towards-closest-reward}: \hspace*{\fill} \tuple{R>0, D>0; D \downarrow}
\end{itemize}

The model is very simple, and can be shown to solve any $\Q_{reward}$ instance irrespective of 
grid size and number and location of rewards and cell blocks, \emph{as long as the instance is solvable},
i.e. all rewards are reachable.
\alert{Interesting point here: how does our approach fare with unsolvability? And with problems with dead-end?}


\alert{Display policy here}

Discuss suboptimality of the policy?
}

% Action 1 [PICK-REWARD]:
% 	PRE: min-dist[at, Restrict(adjacent,Not(blocked)), reward] = 0, num-rewards > 0
% 	EFFS: DEC num-rewards, INC min-dist[at, Restrict(adjacent,Not(blocked)), reward]
% 
% Action 2 [MOVE-TO-CLOSEST-REWARD]:
% 	PRE: min-dist[at, Restrict(adjacent,Not(blocked)), reward] > 0, num-rewards > 0
% 	EFFS: DEC min-dist[at, Restrict(adjacent,Not(blocked)), reward]



\paragraph{Observations.}
We conclude the section with a few general observations:

\begin{enumerate}
\item
\emph{Soundness}: interestingly, abstract actions obtained from a small number of samples in
$\mathcal{S}$ turn out to be sound not just for $\mathcal{S}$ but for the generalized problem $\Q$,
as discussed above. Yet this does not need to be true for the resulting policies to be
correct for $\Q$.
%
Abstract actions $A_F$ do not actually have to be sound for a policy $\pi$
obtained from them to be sound in $\Q$.
Indeed, for the correctness of $\pi$ for solving $\Q$,
it suffices for $A_F$ to be sound \emph{over the states reachable by $\pi$}
in all the instances $P$ of $\Q$.

For example, the abstract action \emph{pick-up-above-$y$} in $\Q_{on}$
is sound relative to the sample set $\mathcal{S}$ used, but not relative to the general
$\Q_{on}$. In any state $s$ where $x$ is a few blocks above $y$ and clear,
there is no concrete action that instantiates \emph{pick-up-above-$y$},
as picking up the topmost block above $y$ would necessarily make feature $X \equiv holding(x)$
true, which the abstract action does not.
%
Keeping track of the information necessary to ensure general correctness would require indeed
a feature set $F$ with higher complexity.
Yet if we start in states where $x$ and $y$ are in different towers,
states as $s$ will never be reached, and hence the policy is correct and solves $\Q_{on}$.


\item
\emph{Grammar and Feature Pool}: the general DL grammar we have described appears to be
adequate to generate a sufficiently expressive pool of features for the examples we use,
but this does not have to be always so.
To come up with a general strategy in the Towers of Hanoi, for example,
the first action to be taken  (whether to move the smallest disk to target or buffer peg)
depends on the number of disks being odd or even,
which cannot be easily expressed with the type of features generated by our grammar.

\item
\emph{Expressive and Computational Limitations}:
The current approach

\alert{*** TO BE FINISHED ***}

The use of primitive \emph{goal predicates} like $on_g$ with denotation
given by the goal conjunctions in the concrete instances, as done in
\cite{martin-geffner:generalized}, could potentially alleviate this
e.g. to be able to generate general policies for all BW problems.


Why pipeline cannot be used to solve arbitrary problems. Eg, logistics.
Que partes explotan? Sizes k and $\abs{\mathcal{S}}$?

* Goal in $\Q$ needs to be expressible in generic terms (but this is not well formalized in our formulation)
* Increasing number of domain parameters kills scalability.
* How to select which are the good parameters if we want arbitrary goals? This is also related to the proper formalization
  of the approach, which we havent done.



%  \item Running times of the SAT solver are negligible.
%  \item Minimization of precondition DNFs
%  \item Theory T
\end{enumerate}


\section{Discussion}

We have introduced a scheme for computing general plans that mixes
learning and planning: a \emph{learner} infers a general abstraction made
up of features and abstract actions by enforcing a notion of soundness
and completeness over a  set of samples, and a \emph{planner}
uses the abstraction, suitably transformed, to compute general plans. 
The number of samples required for obtaining correct general plans is
very small as the learner does not have to learn how to plan, but
just identify the features that are relevant for the  planner to track.
Moreover, unlike, purely learning approaches, the scope and correctness
of the resulting general plans can be assessed.
We have carried out a number of experiments over several domains to 
illustrate the versatility of the approach. In the future, we want 
% explore the use of the learned features and actions in classical planning, to
to study of formal characterization  of the generalized problem $\Q$ and the possibilities of
testing soudness over $\Q$ automatically, and to improve the scalability of the approach,
for example, by collecting the samples intelligently and incrementally
from the states where the computed policy fails, and by reusing the
features and actions obtained for some goal for more complex goals.
The learned features may also have other uses.

\Omit{
** discussion  of experiments; things we do well and not, scope, lessons. limitations (expressive? scalability?)
** briefly related work, expanding paragraph in intro.
** summary and cconlusions
}

\bibliographystyle{aaai}
\bibliography{control}

\end{document}


